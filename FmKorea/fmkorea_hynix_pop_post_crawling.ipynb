{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede7aacb",
   "metadata": {},
   "source": [
    "# FmKorea 하이닉스 커뮤니티 인기글 크롤링 코드\n",
    "\n",
    "### 실행순서\n",
    "\n",
    "1. 1번째 셀 실행 (크롤링을 위한 함수 정의)\n",
    "2. 2번째 셀 실행 (검색 키워드 \"하이닉스\"로 크롤링 실행, df_hynix 데이터프레임 return)\n",
    "3. 3번째 셀 실행 (검색 키워드 \"하닉\"으로 크롤링 실행, df_hanic 데이터프레임 return)\n",
    "4. 4번째 셀 실행 (생성된 df 2개에서 중복되는 게시글 삭제 및 csv 파일생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f387d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/143.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Referer\": \"https://www.fmkorea.com/\",\n",
    "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "}\n",
    "\n",
    "# 날짜 문자열을 정규화(통일)하는 함수\n",
    "# - \"17:27\" 처럼 시간만 있으면 오늘 날짜(YYYY-MM-DD)로 치환\n",
    "# - \"2026.01.12\" 처럼 점(.)으로 된 날짜는 \"-\"로 변경\n",
    "def normalize_date(date_str: str) -> str:\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    s = (date_str or \"\").strip()\n",
    "    if re.match(r\"^\\d{1,2}:\\d{2}$\", s):\n",
    "        return today\n",
    "    if \".\" in s:\n",
    "        return s.replace(\".\", \"-\")\n",
    "    return s\n",
    "\n",
    "# 검색 결과 페이지를 순회하며 게시글 리스트를 수집해서 DataFrame으로 반환\n",
    "# start_page ~ end_page: 수집할 페이지 범위\n",
    "def crawl_one(session: requests.Session, url_base: str, source_name: str,\n",
    "              start_page=1, end_page=10, sleep_sec=2) -> pd.DataFrame:\n",
    "    data = {\"탭\": [], \"제목\": [], \"글쓴이\": [], \"날짜\": [], \"조회\": [], \"추천\": [],\n",
    "            \"post_url\": [], \"source\": []}\n",
    "\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        url = url_base.format(page)\n",
    "\n",
    "        r = session.get(url, timeout=20)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        soup = bs(r.text, \"lxml\")\n",
    "\n",
    "        rows = soup.select(\"table.bd_lst.bd_tb_lst.bd_tb tbody tr\")\n",
    "        if not rows:\n",
    "            print(f\"[{source_name}] {page}페이지: rows=0 → 중단\")\n",
    "            break\n",
    "\n",
    "        page_added = 0\n",
    "        for tr in rows:\n",
    "            cate_a = tr.select_one(\"td.cate span\")\n",
    "            title_a = tr.select_one(\"td.title a.hx\")\n",
    "            author_a = tr.select_one(\"td.author a\")\n",
    "            time_td = tr.select_one(\"td.time\")\n",
    "            mno_tds = tr.select(\"td.m_no\")\n",
    "\n",
    "            if not (cate_a and title_a and author_a and time_td and len(mno_tds) >= 2):\n",
    "                continue\n",
    "\n",
    "            views = mno_tds[0].text.strip()\n",
    "            votes = mno_tds[1].text.strip()\n",
    "\n",
    "            href = title_a.get(\"href\", \"\")\n",
    "            post_url = \"https://www.fmkorea.com\" + href if href.startswith(\"/\") else href\n",
    "\n",
    "            data[\"탭\"].append(cate_a.get_text(strip=True))\n",
    "            data[\"제목\"].append(title_a.get_text(\" \", strip=True))\n",
    "            data[\"글쓴이\"].append(author_a.get_text(strip=True))\n",
    "            data[\"날짜\"].append(normalize_date(time_td.get_text(strip=True)))\n",
    "            data[\"조회\"].append(int(views.replace(\",\", \"\")) if views else 0)\n",
    "            data[\"추천\"].append(int(votes.replace(\",\", \"\")) if votes else 0)\n",
    "            data[\"post_url\"].append(post_url)\n",
    "            data[\"source\"].append(source_name)\n",
    "\n",
    "            page_added += 1\n",
    "\n",
    "\n",
    "        print(f\"[{source_name}] {page}페이지 완료 / 이번 페이지 {page_added}개 / 누적 {len(data['post_url'])}개\")\n",
    "\n",
    "        if page_added == 0:\n",
    "            print(f\"[{source_name}] {page}페이지: page_added=0 → 중단\")\n",
    "            break\n",
    "\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c2d1a",
   "metadata": {},
   "source": [
    "## 크롤링 실행\n",
    "\n",
    "https://www.fmkorea.com/search.php?mid=stock&category=2997203870&listStyle=list&search_keyword=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&search_target=title_content&page=1\n",
    "\n",
    "- 코드 실행시, 위 링크로 접속한뒤에 다시 링크를 복사한뒤 붙여넣어주세요.\n",
    "- 붙여넣고 링크 맨 뒤, page 부분을 {}로 수정해주세요.\n",
    "- 코드 실행시, 페이지 새로고침 후 실행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc38260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색키원드 \"하이닉스\" 인기글\n",
    "\n",
    "url_base_samsung = \"https://www.fmkorea.com/search.php?mid=stock&search_keyword=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&search_target=title_content&sort_index=pop&order_type=asc&listStyle=list&page={}\"\n",
    "\n",
    "# Session을 쓰면 동일한 세션(쿠키 등)을 유지한 채 여러 페이지를 연속 요청할 수 있음 \n",
    "session_samsung = requests.Session()\n",
    "session_samsung.headers.update(headers)  # 세션별 쿠키/헤더 유지\n",
    "#17\n",
    "start_page = 1\n",
    "end_page = 33\n",
    "\n",
    "# 크롤링 실행\n",
    "df_samsung = crawl_one(\n",
    "    session_samsung,\n",
    "    url_base_samsung,\n",
    "    \"삼성전자\",\n",
    "    start_page=start_page,\n",
    "    end_page=end_page\n",
    ")\n",
    "\n",
    "df_samsung.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c47ae78",
   "metadata": {},
   "source": [
    "## 크롤링 실행\n",
    "\n",
    "https://www.fmkorea.com/search.php?mid=stock&category=2997203870&listStyle=list&search_keyword=%EC%82%BC%EC%A0%84&search_target=title_content&page=1\n",
    "\n",
    "- 코드 실행시, 위 링크로 접속한뒤에 다시 링크를 복사한뒤 붙여넣어주세요.\n",
    "- 붙여넣고 링크 맨 뒤, page 부분을 {}로 수정해주세요.\n",
    "- 코드 실행시, 페이지 새로고침 후 실행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색키워드 \"삼전\"\n",
    "\n",
    "url_base_samjeon = \"https://www.fmkorea.com/search.php?mid=stock&sort_index=pop&order_type=desc&listStyle=list&search_target=title_content&search_keyword=%EC%82%BC%EC%A0%84&page={}\"\n",
    "\n",
    "# Session을 쓰면 동일한 세션(쿠키 등)을 유지한 채 여러 페이지를 연속 요청할 수 있음 \n",
    "session_samjeon = requests.Session()\n",
    "session_samjeon.headers.update(headers)  # 세션별 쿠키/헤더 유지\n",
    "\n",
    "start_page = 1\n",
    "end_page = 47\n",
    "\n",
    "# 크롤링 실행\n",
    "df_samjeon = crawl_one(\n",
    "    session_samjeon,\n",
    "    url_base_samjeon,\n",
    "    \"삼전\",\n",
    "    start_page=start_page,\n",
    "    end_page=end_page\n",
    ")\n",
    "\n",
    "df_samjeon.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e1c25",
   "metadata": {},
   "source": [
    "csv 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7adce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 데이터프레임 합치기\n",
    "\n",
    "df_all = pd.concat([df_samsung, df_samjeon], ignore_index=True) \n",
    "print(f\"합치기 전: {len(df_all):,}\")\n",
    "\n",
    "# 중복 제거: URL 기준\n",
    "df_all = df_all.drop_duplicates(subset=[\"post_url\"], keep=\"first\").reset_index(drop=True) \n",
    "print(f\"URL 중복 제거 후: {len(df_all):,}\")\n",
    "\n",
    "# 날짜 내림차순 정렬\n",
    "df_all[\"날짜_dt\"] = pd.to_datetime(df_all[\"날짜\"], errors=\"coerce\")\n",
    "df_all = (\n",
    "    df_all.sort_values(by=\"날짜_dt\", ascending=False, na_position=\"last\")\n",
    "          .drop(columns=[\"날짜_dt\"])\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_all.to_csv(\"1.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e4f6f6",
   "metadata": {},
   "source": [
    "100페이지씩 나누어진 데이터를 하나의 csv로 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) 합칠 CSV 파일들 (현재 폴더의 csv 전부면 이렇게)\n",
    "files = [\"1.csv\"]\n",
    "\n",
    "# 2) 모두 읽어서 합치기\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"합치기 전:\", len(df_all))\n",
    "\n",
    "# 3) 중복 제거 (post_url 기준이 가장 확실)\n",
    "df_all = df_all.drop_duplicates(subset=[\"post_url\"], keep=\"first\").reset_index(drop=True)  # [web:60]\n",
    "print(\"중복 제거 후:\", len(df_all))\n",
    "\n",
    "# 4) 날짜 내림차순 정렬(안전하게 datetime 변환 후 정렬)\n",
    "df_all[\"날짜_dt\"] = pd.to_datetime(df_all[\"날짜\"], errors=\"coerce\")\n",
    "df_all = (\n",
    "    df_all.sort_values(by=\"날짜_dt\", ascending=False, na_position=\"last\")  # [web:102]\n",
    "          .drop(columns=[\"날짜_dt\"])\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 5) 최종 저장\n",
    "df_all.to_csv(\"fm_samsung_pop.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded1f43",
   "metadata": {},
   "source": [
    "## 추출한 게시글 상세정보 (본문내용, 댓글내용) 추출함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def normalize_date(date_str: str) -> str:\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    s = (date_str or \"\").strip()\n",
    "\n",
    "    # 예: \"11:07\" 같은 케이스(인기글에서 시간만 주는 경우) -> 오늘 날짜로\n",
    "    if re.match(r\"^\\d{1,2}:\\d{2}$\", s):\n",
    "        return today\n",
    "\n",
    "    # 예: \"2026.01.16 11:07\" 또는 \"2026.01.16\" -> \"2026-01-16\"\n",
    "    if re.match(r\"^\\d{4}\\.\\d{2}\\.\\d{2}\", s):\n",
    "        s = s.split()[0]          # 시간 잘라내기\n",
    "        return s.replace(\".\", \"-\")\n",
    "\n",
    "    return s\n",
    "\n",
    "def parse_int(text):\n",
    "    # \"조회 수 15,720\" 같은 문자열에서 숫자만 뽑아 int로 변환\n",
    "    if text is None:\n",
    "        return 0\n",
    "    t = re.sub(r\"[^\\d]\", \"\", text)\n",
    "    return int(t) if t else 0\n",
    "\n",
    "def parse_post_detail(session, post_url):\n",
    "    \n",
    "    r = session.get(post_url, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    soup = bs(r.text, \"lxml\")\n",
    "\n",
    "    # 제목\n",
    "    title = \"\"\n",
    "    title_el = soup.select_one(\"#bd_capture h1.np_18px span.np_18px_span\")\n",
    "    if title_el:\n",
    "        title = title_el.get_text(\" \", strip=True)\n",
    "\n",
    "    # 날짜\n",
    "    date = \"\"\n",
    "    date_el = soup.select_one(\"#bd_capture .top_area .date.m_no\")\n",
    "    if date_el:\n",
    "        date = normalize_date(date_el.get_text(strip=True))\n",
    "\n",
    "    # 조회/추천/댓글 수\n",
    "    views = votes = comment_cnt = 0\n",
    "    views_b = soup.select_one(\"#bd_capture .btm_area .side.fr span:nth-of-type(1) b\")\n",
    "    votes_b = soup.select_one(\"#bd_capture .btm_area .side.fr span:nth-of-type(2) b\")\n",
    "    cmt_b   = soup.select_one(\"#bd_capture .btm_area .side.fr span:nth-of-type(3) b\")\n",
    "    views = parse_int(views_b.get_text(strip=True) if views_b else None)\n",
    "    votes = parse_int(votes_b.get_text(strip=True) if votes_b else None)\n",
    "    comment_cnt = parse_int(cmt_b.get_text(strip=True) if cmt_b else None)\n",
    "\n",
    "    # 본문 \n",
    "    content = \"\"\n",
    "    content_el = soup.select_one(\"#bd_capture .rd_body article .xe_content\")\n",
    "    if content_el:\n",
    "        content = content_el.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    # 댓글 목록\n",
    "    comments = []\n",
    "    seen=set()  # (nickname, comment) 중복 체크용\n",
    "    \n",
    "    for li in soup.select(\".fdb_lst_wrp #cmtPosition ul.fdb_lst_ul > li.fdb_itm.clear\"):\n",
    "\n",
    "        # 닉네임: meta 안 a.member_plate 텍스트\n",
    "        nick = \"\"\n",
    "        nick_el = li.select_one(\"div.meta a.member_plate\")\n",
    "        if nick_el:\n",
    "            nick = nick_el.get_text(strip=True)\n",
    "\n",
    "        # 댓글 내용: comment-content 안 xe_content\n",
    "        c_text = \"\"\n",
    "        text_el = li.select_one(\".comment-content .xe_content\")\n",
    "        if text_el:\n",
    "            c_text = text_el.get_text(\"\\n\", strip=True)\n",
    "\n",
    "        # 댓글 추천수: span.voted_count (없으면 0)\n",
    "        like = 0\n",
    "        like_el = li.select_one(\".voted_count\")\n",
    "        if like_el:\n",
    "            like = parse_int(like_el.get_text(strip=True))\n",
    "        \n",
    "        # 닉+내용 완전 동일 중복 제거\n",
    "        key = (nick, c_text)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        \n",
    "        comments.append({\n",
    "            \"nickname\": nick,\n",
    "            \"comment\": c_text,\n",
    "            \"like\": like\n",
    "        })\n",
    "        \n",
    "    return {\n",
    "        \"post_url\": post_url,\n",
    "        \"title\": title,\n",
    "        \"date\": date,\n",
    "        \"views\": views,\n",
    "        \"votes\": votes,\n",
    "        \"comment_count\": comment_cnt,\n",
    "        \"content\": content,\n",
    "        \"comments\": comments\n",
    "    }\n",
    "\n",
    "def append_jsonl(path, obj):\n",
    "    # JSONL(JSON Lines) 파일에 \"한 줄 = JSON 객체 1개\" 형태로 누적 저장하는 함수\n",
    "    # - mode=\"a\": 기존 파일 뒤에 계속 추가(append)\n",
    "    # - ensure_ascii=False: 한글을 \\uXXXX 이스케이프가 아니라 실제 한글로 저장\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")  \n",
    "\n",
    "\n",
    "# JSONL 파일은 사람이 보기엔 1줄이 너무 길어서 불편하니,\n",
    "# JSONL 전체를 읽어서 리스트로 만든 후 pretty JSON(들여쓰기/줄바꿈)으로 저장하는 함수\n",
    "def export_pretty_json(jsonl_path, pretty_json_path):\n",
    "    rows = []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))  # jsonl 한 줄을 dict로\n",
    "\n",
    "    with open(pretty_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rows, f, ensure_ascii=False, indent=2)  # 줄바꿈/들여쓰기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea9214",
   "metadata": {},
   "source": [
    "## 실제 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "df = pd.read_csv(\"fm_samsung_pop.csv\")\n",
    "urls = df[\"post_url\"].dropna().unique()\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "# 한번 요청할때 100페이지 이상은 430에러 발생해서 100페이지씩 나누어서 실행\n",
    "BATCH_SIZE = 100\n",
    "START = 1500  # <- 재실행할 때 0, 100, 200 ... 으로 바꿔가며 실행\n",
    "\n",
    "end = min(START + BATCH_SIZE, len(urls))\n",
    "\n",
    "for i in range(START, end):\n",
    "    raw_url = str(urls[i])\n",
    "\n",
    "    obj = parse_post_detail(session, raw_url)\n",
    "    append_jsonl(\"fmkorea_hot_posts.jsonl\", obj)\n",
    "\n",
    "print(f\"완료: {START} ~ {end-1} / 전체 {len(urls)}\")\n",
    "\n",
    "#jsonl 파일을 pretty JSON으로 변환\n",
    "export_pretty_json(\"fmkorea_hot_posts.jsonl\", \"fmkorea_hot_posts_pretty.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
