{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c395e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from kiwipiepy import Kiwi\n",
    "import pandas as pd\n",
    "\n",
    "jsonl_path = r\"..\\data\\fmkorea_hot_posts.jsonl\"\n",
    "knu_path = r\"..\\data\\KnuSentiLex\\KnuSentiLex\\data\\SentiWord_info.json\"\n",
    "\n",
    "TARGET_DATE = \"2025-11-04\"   # <- 1월 15일(문자열이 파일에 저장된 형식과 같아야 함)\n",
    "\n",
    "def normalize_repeats(s: str) -> str:\n",
    "    s = re.sub(r\"ㅋ{5,}\", \"ㅋㅋㅋㅋ\", s)\n",
    "    s = re.sub(r\"ㅎ{5,}\", \"ㅎㅎㅎㅎ\", s)\n",
    "    s = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", s)\n",
    "    s = re.sub(r\"ㅜ{3,}\", \"ㅜㅜ\", s)\n",
    "    return s\n",
    "\n",
    "# Kiwi\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word(\"삼전\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"삼성전자\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하닉\", \"NNP\", 0)\n",
    "\n",
    "STOP_TAGS = {\n",
    "    \"JKS\",\"JKC\",\"JKG\",\"JKO\",\"JKB\",\"JKV\",\"JKQ\",\"JX\",\"JC\",\n",
    "    \"EP\",\"EF\",\"EC\",\"ETN\",\"ETM\",\n",
    "    \"SF\",\"SP\",\"SS\",\"SE\",\"SO\",\"SW\"\n",
    "}\n",
    "\n",
    "# KNU 로드\n",
    "with open(knu_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knu = json.load(f)\n",
    "\n",
    "lex_score = {}\n",
    "for row in knu:\n",
    "    w = str(row.get(\"word\", \"\")).strip()\n",
    "    if not w:\n",
    "        continue\n",
    "    try:\n",
    "        lex_score[w] = float(row.get(\"polarity\", 0))\n",
    "    except (TypeError, ValueError):\n",
    "        lex_score[w] = 0.0\n",
    "\n",
    "custom_score = {\n",
    "    \"ㅋㅋ\": 0.2, \"ㅋㅋㅋ\": 0.4, \"ㅋㅋㅋㅋ\": 0.6,\n",
    "    \"ㅎㅎ\": 0.2, \"ㅎㅎㅎ\": 0.4, \"ㅎㅎㅎㅎ\": 0.6,\n",
    "    \"떡상\": 2.5, \"폭등\": 2.5,\n",
    "    \"떡락\": -2.5, \"폭락\": -2.5,\n",
    "    \"손절\": -1.5, \"망했다\": -3.0, \"조졌다\": -3.0,\n",
    "    \"고점\": -2.0,\n",
    "}\n",
    "final_lex = {**lex_score, **custom_score}\n",
    "\n",
    "# === 여기부터 '하루만' 집계 ===\n",
    "token_counter = Counter()\n",
    "post_count = 0\n",
    "comment_count = 0\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        post = json.loads(line)\n",
    "\n",
    "        # 1) 날짜 필터\n",
    "        if post.get(\"date\") != TARGET_DATE:\n",
    "            continue\n",
    "\n",
    "        post_count += 1\n",
    "        comments = post.get(\"comments\", [])\n",
    "        comment_count += len(comments)\n",
    "\n",
    "        # 2) 댓글 토큰화 → Counter 누적\n",
    "        for c in comments:\n",
    "            text = normalize_repeats(c.get(\"comment\", \"\"))\n",
    "            tokens = [t.form for t in kiwi.tokenize(text) if t.tag not in STOP_TAGS]\n",
    "            token_counter.update(tokens)\n",
    "\n",
    "# 3) 하루치 감성 점수 계산(유니크 토큰만 매칭)\n",
    "matched_vocab = {w: final_lex[w] for w in token_counter if w in final_lex and float(final_lex[w]) != 0}\n",
    "day_sent_score = sum(token_counter[w] * float(matched_vocab[w]) for w in matched_vocab)\n",
    "label = \"긍정\" if day_sent_score > 0 else \"부정\" if day_sent_score < 0 else \"중립\"\n",
    "\n",
    "print(\"날짜:\", TARGET_DATE)\n",
    "print(\"게시글 수:\", post_count)\n",
    "print(\"댓글 수:\", comment_count)\n",
    "print(\"유니크 토큰 수:\", len(token_counter))\n",
    "print(\"상위 토큰 20개:\", token_counter.most_common(20))\n",
    "print(\"사전 매칭 유니크 단어 수:\", len(matched_vocab))\n",
    "print(\"감성 점수(빈도 가중):\", day_sent_score)\n",
    "print(\"판정:\", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "out_path = r\"..\\data\\token_counter_2026-01-15_all.csv\"\n",
    "\n",
    "# 1) 전부 저장 (유니크 토큰 전체)\n",
    "token_df = pd.DataFrame(token_counter.most_common(), columns=[\"token\", \"count\"])  # [web:366]\n",
    "token_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")  # [web:365]\n",
    "\n",
    "# 2) 저장 확인(파일 존재/크기/행 수/미리보기)\n",
    "print(\"saved:\", out_path)\n",
    "print(\"exists:\", os.path.exists(out_path))\n",
    "print(\"size(bytes):\", os.path.getsize(out_path) if os.path.exists(out_path) else None)\n",
    "print(\"rows(unique tokens):\", len(token_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40462a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from kiwipiepy import Kiwi\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "jsonl_path = r\"..\\data\\fmkorea_hot_posts.jsonl\"\n",
    "knu_path = r\"..\\data\\KnuSentiLex\\KnuSentiLex\\data\\SentiWord_info.json\"\n",
    "\n",
    "TARGET_DATE = \"2025-11-04\"\n",
    "\n",
    "\n",
    "def normalize_repeats(s: str) -> str:\n",
    "    s = re.sub(r\"ㅋ{5,}\", \"ㅋㅋㅋㅋ\", s)\n",
    "    s = re.sub(r\"ㅎ{5,}\", \"ㅎㅎㅎㅎ\", s)\n",
    "    s = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", s)\n",
    "    s = re.sub(r\"ㅜ{3,}\", \"ㅜㅜ\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Kiwi\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word(\"삼전\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"삼성전자\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하닉\", \"NNP\", 0)\n",
    "\n",
    "STOP_TAGS = {\n",
    "    \"JKS\",\"JKC\",\"JKG\",\"JKO\",\"JKB\",\"JKV\",\"JKQ\",\"JX\",\"JC\",\n",
    "    \"EP\",\"EF\",\"EC\",\"ETN\",\"ETM\",\n",
    "    \"SF\",\"SP\",\"SS\",\"SE\",\"SO\",\"SW\"\n",
    "}\n",
    "\n",
    "\n",
    "# KNU 로드\n",
    "with open(knu_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knu = json.load(f)\n",
    "\n",
    "lex_score = {}\n",
    "for row in knu:\n",
    "    w = str(row.get(\"word\", \"\")).strip()\n",
    "    if not w:\n",
    "        continue\n",
    "    try:\n",
    "        lex_score[w] = float(row.get(\"polarity\", 0))\n",
    "    except (TypeError, ValueError):\n",
    "        lex_score[w] = 0.0\n",
    "\n",
    "custom_score = {\n",
    "    \"ㅋㅋ\": 0.2, \"ㅋㅋㅋ\": 0.4, \"ㅋㅋㅋㅋ\": 0.6,\n",
    "    \"ㅎㅎ\": 0.2, \"ㅎㅎㅎ\": 0.4, \"ㅎㅎㅎㅎ\": 0.6,\n",
    "    \"떡상\": 2.5, \"폭등\": 2.5,\n",
    "    \"떡락\": -2.5, \"폭락\": -2.5,\n",
    "    \"손절\": -1.5, \"망했다\": -3.0, \"조졌다\": -3.0,\n",
    "    \"고점\": -2.0,\n",
    "}\n",
    "final_lex = {**lex_score, **custom_score}\n",
    "\n",
    "\n",
    "# ====== (추가) HF 3분류 모델 로드 ======\n",
    "MODEL_NAME = \"jbeno/electra-base-classifier-sentiment\"\n",
    "clf = pipeline(\"text-classification\", model=MODEL_NAME, top_k=None, truncation=True)  # 3라벨 확률 [web:129][web:119]\n",
    "\n",
    "\n",
    "def scores_to_dict(out):\n",
    "    # out: [{'label':'negative','score':...}, ...]\n",
    "    d = {x[\"label\"].lower(): float(x[\"score\"]) for x in out}\n",
    "    # 모델마다 라벨 대소문자/형태 차이가 있을 수 있어 방어적으로 처리\n",
    "    return {\n",
    "        \"negative\": d.get(\"negative\", 0.0),\n",
    "        \"neutral\": d.get(\"neutral\", 0.0),\n",
    "        \"positive\": d.get(\"positive\", 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "def label_from_probs(p):\n",
    "    # p: {'negative':..., 'neutral':..., 'positive':...}\n",
    "    best = max(p, key=p.get)\n",
    "    return {\"negative\": \"부정\", \"neutral\": \"중립\", \"positive\": \"긍정\"}[best]\n",
    "\n",
    "\n",
    "def like_weight(like):\n",
    "    # 0~큰 값까지 안정적으로: 1 + log(1+like)\n",
    "    like = 0 if like is None else int(like)\n",
    "    return 1.0 + math.log1p(max(like, 0))\n",
    "\n",
    "\n",
    "# === 하루치 집계 ===\n",
    "token_counter = Counter()\n",
    "post_count = 0\n",
    "comment_count = 0\n",
    "\n",
    "# (추가) AI 집계용\n",
    "ai_weight_sum = 0.0\n",
    "ai_sum = {\"negative\": 0.0, \"neutral\": 0.0, \"positive\": 0.0}\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        post = json.loads(line)\n",
    "\n",
    "        if post.get(\"date\") != TARGET_DATE:\n",
    "            continue\n",
    "\n",
    "        post_count += 1\n",
    "        comments = post.get(\"comments\", [])\n",
    "        comment_count += len(comments)\n",
    "\n",
    "        # 1) 댓글 토큰화 → Counter 누적 (기존)\n",
    "        for c in comments:\n",
    "            text = normalize_repeats(c.get(\"comment\", \"\"))\n",
    "\n",
    "            tokens = [t.form for t in kiwi.tokenize(text) if t.tag not in STOP_TAGS]\n",
    "            token_counter.update(tokens)\n",
    "\n",
    "        # 2) (추가) 댓글을 AI로 감성분석 후 like 가중 합산\n",
    "        comment_texts = [normalize_repeats(c.get(\"comment\", \"\")) for c in comments if c.get(\"comment\")]\n",
    "        if comment_texts:\n",
    "            # batch 추론: 리스트로 넣으면 배치 처리됨 [web:99]\n",
    "            outs = clf(comment_texts, top_k=None)  # 버전에 따라 중첩 리스트/단일 리스트 형태가 달라질 수 있음 [web:106][web:129]\n",
    "\n",
    "            # outs 정규화: comment_texts 길이만큼 '각 댓글의 out(list[dict])'로 맞추기\n",
    "            if outs and isinstance(outs[0], dict):\n",
    "                # 텍스트 1개만 들어갔을 때 dict 리스트로 오는 케이스 방어\n",
    "                outs = [outs]\n",
    "\n",
    "            for c, out in zip(comments, outs):\n",
    "                p = scores_to_dict(out)\n",
    "                w = like_weight(c.get(\"like\", 0))\n",
    "                ai_sum[\"negative\"] += w * p[\"negative\"]\n",
    "                ai_sum[\"neutral\"] += w * p[\"neutral\"]\n",
    "                ai_sum[\"positive\"] += w * p[\"positive\"]\n",
    "                ai_weight_sum += w\n",
    "\n",
    "\n",
    "# 3) (기존) 하루치 사전 감성 점수\n",
    "matched_vocab = {w: final_lex[w] for w in token_counter if w in final_lex and float(final_lex[w]) != 0}\n",
    "day_sent_score = sum(token_counter[w] * float(matched_vocab[w]) for w in matched_vocab)\n",
    "lex_label = \"긍정\" if day_sent_score > 0 else \"부정\" if day_sent_score < 0 else \"중립\"\n",
    "\n",
    "# 4) (추가) 하루치 AI 확률/라벨\n",
    "if ai_weight_sum > 0:\n",
    "    ai_probs = {k: v / ai_weight_sum for k, v in ai_sum.items()}\n",
    "    ai_label = label_from_probs(ai_probs)\n",
    "else:\n",
    "    ai_probs = {\"negative\": 0.0, \"neutral\": 0.0, \"positive\": 0.0}\n",
    "    ai_label = \"중립\"\n",
    "\n",
    "# 5) (옵션) 하이브리드: AI가 애매할 때만 사전으로 보정(원하면 임계값 조정)\n",
    "ai_conf = max(ai_probs.values())\n",
    "if ai_conf < 0.50:\n",
    "    final_label = lex_label\n",
    "else:\n",
    "    final_label = ai_label\n",
    "\n",
    "\n",
    "print(\"날짜:\", TARGET_DATE)\n",
    "print(\"게시글 수:\", post_count)\n",
    "print(\"댓글 수:\", comment_count)\n",
    "print(\"유니크 토큰 수:\", len(token_counter))\n",
    "print(\"상위 토큰 20개:\", token_counter.most_common(20))\n",
    "print(\"사전 매칭 유니크 단어 수:\", len(matched_vocab))\n",
    "print(\"사전 감성 점수(빈도 가중):\", day_sent_score)\n",
    "print(\"사전 판정:\", lex_label)\n",
    "\n",
    "print(\"\\n[AI 모델]\", MODEL_NAME)\n",
    "print(\"AI 확률(가중평균):\", ai_probs)\n",
    "print(\"AI 판정:\", ai_label)\n",
    "print(\"최종(하이브리드) 판정:\", final_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfc313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U \"transformers[torch]\" huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ef27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "import pandas as pd\n",
    "\n",
    "jsonl_path = r\"..\\data\\fmkorea_samsung_hot_posts.jsonl\"\n",
    "\n",
    "START_DATE = pd.to_datetime(\"2026-01-01\").date()\n",
    "END_DATE   = pd.to_datetime(\"2026-01-16\").date()\n",
    "\n",
    "OUT_CSV = r\"..\\완료\\daily_outputs\\fmkorea_tokens_daily_2025-01-14_2026-01-14.csv\"\n",
    "\n",
    "\n",
    "def normalize_repeats(s: str) -> str:\n",
    "    s = re.sub(r\"ㅋ{5,}\", \"ㅋㅋㅋㅋ\", s)\n",
    "    s = re.sub(r\"ㅎ{5,}\", \"ㅎㅎㅎㅎ\", s)\n",
    "    s = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", s)\n",
    "    s = re.sub(r\"ㅜ{3,}\", \"ㅜㅜ\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Kiwi\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word(\"삼전\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"삼성전자\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하닉\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하이닉스\", \"NNP\", 0)\n",
    "\n",
    "STOP_TAGS = {\n",
    "    \"JKS\",\"JKC\",\"JKG\",\"JKO\",\"JKB\",\"JKV\",\"JKQ\",\"JX\",\"JC\",\n",
    "    \"EP\",\"EF\",\"EC\",\"ETN\",\"ETM\",\n",
    "    \"SF\",\"SP\",\"SS\",\"SE\",\"SO\",\"SW\"\n",
    "}\n",
    "\n",
    "def tokenize_text(text: str) -> list[str]:\n",
    "    \"\"\"본문/댓글 공통 토큰화: 반복 문자 정규화 + 품사 필터\"\"\"\n",
    "    text = normalize_repeats(text or \"\")\n",
    "    return [t.form for t in kiwi.tokenize(text) if t.tag not in STOP_TAGS]  # kiwi.tokenize 사용 [web:73]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 날짜별 누적 저장소\n",
    "# =========================\n",
    "daily_post_count = defaultdict(int)\n",
    "daily_comment_count = defaultdict(int)\n",
    "daily_token_counter = defaultdict(Counter)\n",
    "\n",
    "# (선택) 너무 커지면 끄기: 날짜별 상위 토큰만 뽑아 저장할 때 유용\n",
    "# daily_total_token = defaultdict(int)\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        post = json.loads(line)\n",
    "\n",
    "        # 1) 날짜 파싱/필터\n",
    "        date_str = post.get(\"date\")\n",
    "        if not date_str:\n",
    "            continue\n",
    "\n",
    "        d = pd.to_datetime(date_str, errors=\"coerce\")\n",
    "        if pd.isna(d):\n",
    "            continue\n",
    "\n",
    "        d = d.date()\n",
    "        if d < START_DATE or d > END_DATE:\n",
    "            continue\n",
    "\n",
    "        # 2) 본문 + 댓글 토큰화\n",
    "        daily_post_count[d] += 1\n",
    "\n",
    "        # ✅ 본문 키 이름은 데이터에 맞게 조정 가능: content/body/text 중 하나일 수 있음\n",
    "        body = post.get(\"content\") or post.get(\"body\") or post.get(\"text\") or \"\"\n",
    "        daily_token_counter[d].update(tokenize_text(body))\n",
    "\n",
    "        comments = post.get(\"comments\", [])\n",
    "        daily_comment_count[d] += len(comments)\n",
    "\n",
    "        for c in comments:\n",
    "            # 네 원본 코드: c.get(\"comment\")\n",
    "            c_text = \"\"\n",
    "            if isinstance(c, dict):\n",
    "                c_text = c.get(\"comment\", \"\") or c.get(\"content\", \"\") or c.get(\"text\", \"\")\n",
    "            else:\n",
    "                c_text = str(c)\n",
    "            daily_token_counter[d].update(tokenize_text(c_text))\n",
    "\n",
    "# =========================\n",
    "# 일별 DataFrame 생성(빈 날짜 0 포함)\n",
    "# =========================\n",
    "rows = []\n",
    "for d in pd.date_range(START_DATE, END_DATE, freq=\"D\").date:\n",
    "    counter = daily_token_counter.get(d, Counter())\n",
    "    total_tokens = sum(counter.values())\n",
    "    unique_tokens = len(counter)\n",
    "\n",
    "    rows.append({\n",
    "        \"날짜\": pd.to_datetime(d).strftime(\"%Y-%m-%d\"),\n",
    "        \"게시글수\": int(daily_post_count.get(d, 0)),\n",
    "        \"댓글수\": int(daily_comment_count.get(d, 0)),\n",
    "        \"총토큰수\": int(total_tokens),\n",
    "        \"유니크토큰수\": int(unique_tokens),\n",
    "        \"상위토큰20\": counter.most_common(20),  # most_common 사용 [web:386]\n",
    "    })\n",
    "\n",
    "daily_df = pd.DataFrame(rows)\n",
    "daily_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[저장]\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df066f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install electra-classifier transformers torch kiwipiepy pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c751119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall -y torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a92755",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U transformers torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430522e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "name = \"snunlp/KR-FinBert-SC\"\n",
    "AutoTokenizer.from_pretrained(name)\n",
    "AutoModelForSequenceClassification.from_pretrained(name)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "JSONL_PATH = r\"..\\data\\fmkorea_samsung_hot_posts.jsonl\"\n",
    "OUT_DIR = r\"..\\output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"daily_fng_3class.csv\")\n",
    "\n",
    "START_DATE = \"2025-11-05\"\n",
    "END_DATE   = \"2026-01-14\"\n",
    "\n",
    "MODEL_NAME = \"snunlp/KR-FinBert-SC\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 모델 로드\n",
    "# =========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "id2label = {int(k): str(v).lower() for k, v in model.config.id2label.items()}  # 모델 라벨 확인용 [web:467]\n",
    "\n",
    "\n",
    "def probs_to_fng(probs_dict: dict):\n",
    "    # label 이름이 pos/neg/neu로 포함되는지로 안전 매핑 [web:467]\n",
    "    def pick(keys):\n",
    "        for k, v in probs_dict.items():\n",
    "            lk = k.lower()\n",
    "            if any(key in lk for key in keys):\n",
    "                return float(v)\n",
    "        return 0.0\n",
    "\n",
    "    neg = pick([\"neg\"])\n",
    "    neu = pick([\"neu\"])\n",
    "    pos = pick([\"pos\"])\n",
    "\n",
    "    # 3클래스 결정\n",
    "    cls = np.argmax([neg, neu, pos])\n",
    "    if cls == 0:\n",
    "        label = \"fear\"      # 공포\n",
    "        score = -10.0\n",
    "    elif cls == 1:\n",
    "        label = \"neutral\"   # 중립\n",
    "        score = 0.0\n",
    "    else:\n",
    "        label = \"greed\"     # 탐욕\n",
    "        score = 10.0\n",
    "    return label, score, (neg, neu, pos)\n",
    "\n",
    "\n",
    "def infer_one(text: str):\n",
    "    if not text or not str(text).strip():\n",
    "        return None\n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, max_length=512).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "    probs_dict = {id2label[i]: float(probs[i]) for i in range(len(probs))}\n",
    "    return probs_to_fng(probs_dict)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 날짜별 집계\n",
    "# =========================\n",
    "start_d = pd.to_datetime(START_DATE).date()\n",
    "end_d   = pd.to_datetime(END_DATE).date()\n",
    "\n",
    "cnt = defaultdict(lambda: {\"fear\": 0, \"neutral\": 0, \"greed\": 0, \"n_texts\": 0})\n",
    "score_sum = defaultdict(float)  # -1/0/1 합(중립=0)\n",
    "\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        post = json.loads(line)\n",
    "\n",
    "        d = pd.to_datetime(post.get(\"date\"), errors=\"coerce\")\n",
    "        if pd.isna(d):\n",
    "            continue\n",
    "        d = d.date()\n",
    "        if d < start_d or d > end_d:\n",
    "            continue\n",
    "\n",
    "        texts = []\n",
    "        texts.append(post.get(\"title\", \"\"))\n",
    "        texts.append(post.get(\"content\", \"\"))\n",
    "        for c in post.get(\"comments\", []) or []:\n",
    "            if isinstance(c, dict):\n",
    "                texts.append(c.get(\"comment\", \"\"))\n",
    "\n",
    "        for t in texts:\n",
    "            out = infer_one(t)\n",
    "            if out is None:\n",
    "                continue\n",
    "            label, s, _ = out\n",
    "            cnt[d][label] += 1\n",
    "            cnt[d][\"n_texts\"] += 1\n",
    "            score_sum[d] += s\n",
    "\n",
    "\n",
    "rows = []\n",
    "for d in pd.date_range(start_d, end_d, freq=\"D\").date:\n",
    "    n = cnt[d][\"n_texts\"]\n",
    "    fear = cnt[d][\"fear\"]\n",
    "    neu  = cnt[d][\"neutral\"]\n",
    "    greed = cnt[d][\"greed\"]\n",
    "\n",
    "    # -1~1 평균 (중립은 0)\n",
    "    avg = 0.0 if n == 0 else score_sum[d] / n\n",
    "\n",
    "    rows.append({\n",
    "        \"date\": pd.to_datetime(d).strftime(\"%Y-%m-%d\"),\n",
    "        \"fear_cnt\": fear,\n",
    "        \"neutral_cnt\": neu,\n",
    "        \"greed_cnt\": greed,\n",
    "        \"n_texts\": n,\n",
    "        \"fg_minus1_to_1\": float(avg),\n",
    "        \"fg_0_100\": float((avg + 1.0) * 50.0),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[SAVE]\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07aaf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 경로/설정\n",
    "# =========================\n",
    "JSONL_PATH = r\"..\\data\\fmkorea_samsung_hot_posts.jsonl\"  \n",
    "OUT_DIR = r\"..\\output_wordcloud\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "FONT_PATH = r\"C:\\Windows\\Fonts\\malgun.ttf\" \n",
    "\n",
    "\n",
    "START_DATE = \"2025-07-30\"   # 포함\n",
    "END_DATE   = \"2025-07-30\"   # 포함\n",
    "\n",
    "SAVE_IMG = True\n",
    "SHOW_IMG = True  # 주피터에서 보고 싶으면 True\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 유틸\n",
    "# =========================\n",
    "def normalize_repeats(s: str) -> str:\n",
    "    s = re.sub(r\"ㅋ{5,}\", \"ㅋㅋㅋㅋ\", s)\n",
    "    s = re.sub(r\"ㅎ{5,}\", \"ㅎㅎㅎㅎ\", s)\n",
    "    s = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", s)\n",
    "    s = re.sub(r\"ㅜ{3,}\", \"ㅜㅜ\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Kiwi 세팅\n",
    "# =========================\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word(\"삼전\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"삼성전자\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하닉\", \"NNP\", 0)\n",
    "\n",
    "STOP_TAGS = {\n",
    "    \"JKS\",\"JKC\",\"JKG\",\"JKO\",\"JKB\",\"JKV\",\"JKQ\",\"JX\",\"JC\",\n",
    "    \"EP\",\"EF\",\"EC\",\"ETN\",\"ETM\",\n",
    "    \"SF\",\"SP\",\"SS\",\"SE\",\"SO\",\"SW\"\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize_kiwi(text: str):\n",
    "    \"\"\"불용 품사 제거 + 1글자 토큰 제외\"\"\"\n",
    "    text = normalize_repeats(str(text))\n",
    "    out = []\n",
    "    for t in kiwi.tokenize(text):\n",
    "        if t.tag in STOP_TAGS:\n",
    "            continue\n",
    "        w = t.form.strip()\n",
    "        if len(w) <= 1:\n",
    "            continue\n",
    "        out.append(w)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 날짜 범위\n",
    "# =========================\n",
    "start_d = pd.to_datetime(START_DATE).date()\n",
    "end_d   = pd.to_datetime(END_DATE).date()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 집계 (구간 전체 1장)\n",
    "# =========================\n",
    "token_counter = Counter()\n",
    "post_count = 0\n",
    "comment_count = 0\n",
    "\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        post = json.loads(line)\n",
    "\n",
    "        d = pd.to_datetime(post.get(\"date\"), errors=\"coerce\")\n",
    "        if pd.isna(d):\n",
    "            continue\n",
    "        d = d.date()\n",
    "        if d < start_d or d > end_d:\n",
    "            continue\n",
    "\n",
    "        post_count += 1\n",
    "\n",
    "        # 제목/본문/댓글 모두 포함 (구조 확인됨) [file:40]\n",
    "        texts = []\n",
    "        texts.append(post.get(\"title\", \"\"))\n",
    "        texts.append(post.get(\"content\", \"\"))\n",
    "\n",
    "        comments = post.get(\"comments\", []) or []\n",
    "        comment_count += len(comments)\n",
    "\n",
    "        for c in comments:\n",
    "            if isinstance(c, dict):\n",
    "                texts.append(c.get(\"comment\", \"\"))\n",
    "\n",
    "        # 토큰화 누적\n",
    "        for t in texts:\n",
    "            if not t or not str(t).strip():\n",
    "                continue\n",
    "            token_counter.update(tokenize_kiwi(t))\n",
    "\n",
    "\n",
    "print(\"기간:\", START_DATE, \"~\", END_DATE)\n",
    "print(\"게시글 수:\", post_count)\n",
    "print(\"댓글 수:\", comment_count)\n",
    "print(\"유니크 토큰 수:\", len(token_counter))\n",
    "print(\"상위 토큰 20개:\", token_counter.most_common(20))\n",
    "\n",
    "if len(token_counter) == 0:\n",
    "    raise ValueError(\"해당 날짜 구간에 토큰이 없습니다. 날짜/파일/필터를 확인하세요.\")\n",
    "\n",
    "# =========================\n",
    "# 워드클라우드 생성/저장\n",
    "# =========================\n",
    "wc = WordCloud(\n",
    "    font_path=FONT_PATH,\n",
    "    width=1400,\n",
    "    height=900,\n",
    "    background_color=\"white\",\n",
    "    max_words=200\n",
    ").generate_from_frequencies(dict(token_counter))  # Counter -> dict [web:56]\n",
    "\n",
    "out_png = os.path.join(OUT_DIR, f\"wordcloud_{START_DATE}_{END_DATE}.png\")\n",
    "if SAVE_IMG:\n",
    "    wc.to_file(out_png)  # 파일 저장 [web:56]\n",
    "    print(\"[SAVE]\", out_png)\n",
    "\n",
    "if SHOW_IMG:\n",
    "    plt.figure(figsize=(14, 9))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eaa5fa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-24] posts=5, comments=120, vocab=394\n",
      "  - save csv: ..\\output_wordcloud_by_dates\\wordfreq_2025-12-24.csv\n",
      "  - save png: ..\\output_wordcloud_by_dates\\wordcloud_2025-12-24.png\n",
      "[2026-01-12] posts=12, comments=232, vocab=1260\n",
      "  - save csv: ..\\output_wordcloud_by_dates\\wordfreq_2026-01-12.csv\n",
      "  - save png: ..\\output_wordcloud_by_dates\\wordcloud_2026-01-12.png\n",
      "[2025-09-30] posts=5, comments=144, vocab=347\n",
      "  - save csv: ..\\output_wordcloud_by_dates\\wordfreq_2025-09-30.csv\n",
      "  - save png: ..\\output_wordcloud_by_dates\\wordcloud_2025-09-30.png\n",
      "[2025-03-27] posts=4, comments=67, vocab=484\n",
      "  - save csv: ..\\output_wordcloud_by_dates\\wordfreq_2025-03-27.csv\n",
      "  - save png: ..\\output_wordcloud_by_dates\\wordcloud_2025-03-27.png\n",
      "[2025-07-30] posts=14, comments=410, vocab=857\n",
      "  - save csv: ..\\output_wordcloud_by_dates\\wordfreq_2025-07-30.csv\n",
      "  - save png: ..\\output_wordcloud_by_dates\\wordcloud_2025-07-30.png\n",
      "[2025-11-03] posts=13, comments=314, vocab=1056\n",
      "  - save csv: ..\\output_wordcloud_by_dates\\wordfreq_2025-11-03.csv\n",
      "  - save png: ..\\output_wordcloud_by_dates\\wordcloud_2025-11-03.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "JSONL_PATH = r\"..\\data\\fmkorea_samsung_hot_posts.jsonl\"\n",
    "OUT_DIR = r\"..\\output_wordcloud_by_dates\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "FONT_PATH = r\"C:\\Windows\\Fonts\\malgun.ttf\"\n",
    "\n",
    "# 여기만 바꿔서 원하는 날짜를 리스트로 입력\n",
    "TARGET_DATES = [\n",
    "    \"2025-12-24\",\n",
    "    \"2026-01-12\",\n",
    "    \"2025-09-30\",\n",
    "    \"2025-03-27\",\n",
    "    \"2025-07-30\",\n",
    "    \"2025-11-03\",\n",
    "]\n",
    "\n",
    "TOP_N = 300  # CSV에 저장할 상위 단어 개수(전체 저장하려면 None)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 전처리/토큰화\n",
    "# =========================\n",
    "def normalize_repeats(s: str) -> str:\n",
    "    s = re.sub(r\"ㅋ{5,}\", \"ㅋㅋㅋㅋ\", s)\n",
    "    s = re.sub(r\"ㅎ{5,}\", \"ㅎㅎㅎㅎ\", s)\n",
    "    s = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", s)\n",
    "    s = re.sub(r\"ㅜ{3,}\", \"ㅜㅜ\", s)\n",
    "    return s\n",
    "\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word(\"삼전\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"삼성전자\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하닉\", \"NNP\", 0)\n",
    "\n",
    "STOP_TAGS = {\n",
    "    \"JKS\",\"JKC\",\"JKG\",\"JKO\",\"JKB\",\"JKV\",\"JKQ\",\"JX\",\"JC\",\n",
    "    \"EP\",\"EF\",\"EC\",\"ETN\",\"ETM\",\n",
    "    \"SF\",\"SP\",\"SS\",\"SE\",\"SO\",\"SW\"\n",
    "}\n",
    "\n",
    "def tokenize_kiwi(text: str):\n",
    "    text = normalize_repeats(str(text))\n",
    "    out = []\n",
    "    for t in kiwi.tokenize(text):\n",
    "        if t.tag in STOP_TAGS:\n",
    "            continue\n",
    "        w = t.form.strip()\n",
    "        if len(w) <= 1:      # 1글자 제외\n",
    "            continue\n",
    "        out.append(w)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 날짜별 Counter 만들기\n",
    "# =========================\n",
    "target_set = set(TARGET_DATES)\n",
    "counters = {d: Counter() for d in TARGET_DATES}\n",
    "post_cnt = Counter()\n",
    "comment_cnt = Counter()\n",
    "\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        post = json.loads(line)\n",
    "\n",
    "        d = str(post.get(\"date\", \"\")).strip()\n",
    "        if d not in target_set:\n",
    "            continue\n",
    "\n",
    "        post_cnt[d] += 1\n",
    "\n",
    "        texts = []\n",
    "        texts.append(post.get(\"title\", \"\"))\n",
    "        texts.append(post.get(\"content\", \"\"))\n",
    "\n",
    "        comments = post.get(\"comments\", []) or []\n",
    "        comment_cnt[d] += len(comments)\n",
    "        for c in comments:\n",
    "            if isinstance(c, dict):\n",
    "                texts.append(c.get(\"comment\", \"\"))\n",
    "\n",
    "        for t in texts:\n",
    "            if not t or not str(t).strip():\n",
    "                continue\n",
    "            counters[d].update(tokenize_kiwi(t))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 날짜별 CSV + 워드클라우드 저장\n",
    "# =========================\n",
    "for d in TARGET_DATES:\n",
    "    counter = counters[d]\n",
    "\n",
    "    print(f\"[{d}] posts={post_cnt[d]}, comments={comment_cnt[d]}, vocab={len(counter)}\")\n",
    "\n",
    "    if len(counter) == 0:\n",
    "        print(f\"  - skip: 토큰 없음\")\n",
    "        continue\n",
    "\n",
    "    # 1) 단어빈도 CSV 저장 (Counter -> most_common) [web:74]\n",
    "    rows = counter.most_common(TOP_N) if TOP_N else counter.most_common()\n",
    "    df = pd.DataFrame(rows, columns=[\"word\", \"count\"])\n",
    "    out_csv = os.path.join(OUT_DIR, f\"wordfreq_{d}.csv\")\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"  - save csv:\", out_csv)\n",
    "\n",
    "    # 2) 워드클라우드 PNG 저장\n",
    "    wc = WordCloud(\n",
    "        font_path=FONT_PATH,\n",
    "        width=1400,\n",
    "        height=900,\n",
    "        background_color=\"white\",\n",
    "        max_words=200,\n",
    "    ).generate_from_frequencies(dict(counter))\n",
    "\n",
    "    out_png = os.path.join(OUT_DIR, f\"wordcloud_{d}.png\")\n",
    "    wc.to_file(out_png)\n",
    "    print(\"  - save png:\", out_png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
