{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c395e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from kiwipiepy import Kiwi\n",
    "import pandas as pd\n",
    "\n",
    "jsonl_path = r\"..\\data\\fmkorea_hot_posts.jsonl\"\n",
    "knu_path = r\"..\\data\\KnuSentiLex\\KnuSentiLex\\data\\SentiWord_info.json\"\n",
    "\n",
    "TARGET_DATE = \"2025-11-04\"   # <- 1월 15일(문자열이 파일에 저장된 형식과 같아야 함)\n",
    "\n",
    "def normalize_repeats(s: str) -> str:\n",
    "    s = re.sub(r\"ㅋ{5,}\", \"ㅋㅋㅋㅋ\", s)\n",
    "    s = re.sub(r\"ㅎ{5,}\", \"ㅎㅎㅎㅎ\", s)\n",
    "    s = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", s)\n",
    "    s = re.sub(r\"ㅜ{3,}\", \"ㅜㅜ\", s)\n",
    "    return s\n",
    "\n",
    "# Kiwi\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word(\"삼전\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"삼성전자\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하닉\", \"NNP\", 0)\n",
    "\n",
    "STOP_TAGS = {\n",
    "    \"JKS\",\"JKC\",\"JKG\",\"JKO\",\"JKB\",\"JKV\",\"JKQ\",\"JX\",\"JC\",\n",
    "    \"EP\",\"EF\",\"EC\",\"ETN\",\"ETM\",\n",
    "    \"SF\",\"SP\",\"SS\",\"SE\",\"SO\",\"SW\"\n",
    "}\n",
    "\n",
    "# KNU 로드\n",
    "with open(knu_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knu = json.load(f)\n",
    "\n",
    "lex_score = {}\n",
    "for row in knu:\n",
    "    w = str(row.get(\"word\", \"\")).strip()\n",
    "    if not w:\n",
    "        continue\n",
    "    try:\n",
    "        lex_score[w] = float(row.get(\"polarity\", 0))\n",
    "    except (TypeError, ValueError):\n",
    "        lex_score[w] = 0.0\n",
    "\n",
    "custom_score = {\n",
    "    \"ㅋㅋ\": 0.2, \"ㅋㅋㅋ\": 0.4, \"ㅋㅋㅋㅋ\": 0.6,\n",
    "    \"ㅎㅎ\": 0.2, \"ㅎㅎㅎ\": 0.4, \"ㅎㅎㅎㅎ\": 0.6,\n",
    "    \"떡상\": 2.5, \"폭등\": 2.5,\n",
    "    \"떡락\": -2.5, \"폭락\": -2.5,\n",
    "    \"손절\": -1.5, \"망했다\": -3.0, \"조졌다\": -3.0,\n",
    "    \"고점\": -2.0,\n",
    "}\n",
    "final_lex = {**lex_score, **custom_score}\n",
    "\n",
    "# === 여기부터 '하루만' 집계 ===\n",
    "token_counter = Counter()\n",
    "post_count = 0\n",
    "comment_count = 0\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        post = json.loads(line)\n",
    "\n",
    "        # 1) 날짜 필터\n",
    "        if post.get(\"date\") != TARGET_DATE:\n",
    "            continue\n",
    "\n",
    "        post_count += 1\n",
    "        comments = post.get(\"comments\", [])\n",
    "        comment_count += len(comments)\n",
    "\n",
    "        # 2) 댓글 토큰화 → Counter 누적\n",
    "        for c in comments:\n",
    "            text = normalize_repeats(c.get(\"comment\", \"\"))\n",
    "            tokens = [t.form for t in kiwi.tokenize(text) if t.tag not in STOP_TAGS]\n",
    "            token_counter.update(tokens)\n",
    "\n",
    "# 3) 하루치 감성 점수 계산(유니크 토큰만 매칭)\n",
    "matched_vocab = {w: final_lex[w] for w in token_counter if w in final_lex and float(final_lex[w]) != 0}\n",
    "day_sent_score = sum(token_counter[w] * float(matched_vocab[w]) for w in matched_vocab)\n",
    "label = \"긍정\" if day_sent_score > 0 else \"부정\" if day_sent_score < 0 else \"중립\"\n",
    "\n",
    "print(\"날짜:\", TARGET_DATE)\n",
    "print(\"게시글 수:\", post_count)\n",
    "print(\"댓글 수:\", comment_count)\n",
    "print(\"유니크 토큰 수:\", len(token_counter))\n",
    "print(\"상위 토큰 20개:\", token_counter.most_common(20))\n",
    "print(\"사전 매칭 유니크 단어 수:\", len(matched_vocab))\n",
    "print(\"감성 점수(빈도 가중):\", day_sent_score)\n",
    "print(\"판정:\", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "out_path = r\"..\\data\\token_counter_2026-01-15_all.csv\"\n",
    "\n",
    "# 1) 전부 저장 (유니크 토큰 전체)\n",
    "token_df = pd.DataFrame(token_counter.most_common(), columns=[\"token\", \"count\"])  # [web:366]\n",
    "token_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")  # [web:365]\n",
    "\n",
    "# 2) 저장 확인(파일 존재/크기/행 수/미리보기)\n",
    "print(\"saved:\", out_path)\n",
    "print(\"exists:\", os.path.exists(out_path))\n",
    "print(\"size(bytes):\", os.path.getsize(out_path) if os.path.exists(out_path) else None)\n",
    "print(\"rows(unique tokens):\", len(token_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40462a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from kiwipiepy import Kiwi\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "jsonl_path = r\"..\\data\\fmkorea_hot_posts.jsonl\"\n",
    "knu_path = r\"..\\data\\KnuSentiLex\\KnuSentiLex\\data\\SentiWord_info.json\"\n",
    "\n",
    "TARGET_DATE = \"2025-11-04\"\n",
    "\n",
    "\n",
    "def normalize_repeats(s: str) -> str:\n",
    "    s = re.sub(r\"ㅋ{5,}\", \"ㅋㅋㅋㅋ\", s)\n",
    "    s = re.sub(r\"ㅎ{5,}\", \"ㅎㅎㅎㅎ\", s)\n",
    "    s = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", s)\n",
    "    s = re.sub(r\"ㅜ{3,}\", \"ㅜㅜ\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Kiwi\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word(\"삼전\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"삼성전자\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하닉\", \"NNP\", 0)\n",
    "\n",
    "STOP_TAGS = {\n",
    "    \"JKS\",\"JKC\",\"JKG\",\"JKO\",\"JKB\",\"JKV\",\"JKQ\",\"JX\",\"JC\",\n",
    "    \"EP\",\"EF\",\"EC\",\"ETN\",\"ETM\",\n",
    "    \"SF\",\"SP\",\"SS\",\"SE\",\"SO\",\"SW\"\n",
    "}\n",
    "\n",
    "\n",
    "# KNU 로드\n",
    "with open(knu_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    knu = json.load(f)\n",
    "\n",
    "lex_score = {}\n",
    "for row in knu:\n",
    "    w = str(row.get(\"word\", \"\")).strip()\n",
    "    if not w:\n",
    "        continue\n",
    "    try:\n",
    "        lex_score[w] = float(row.get(\"polarity\", 0))\n",
    "    except (TypeError, ValueError):\n",
    "        lex_score[w] = 0.0\n",
    "\n",
    "custom_score = {\n",
    "    \"ㅋㅋ\": 0.2, \"ㅋㅋㅋ\": 0.4, \"ㅋㅋㅋㅋ\": 0.6,\n",
    "    \"ㅎㅎ\": 0.2, \"ㅎㅎㅎ\": 0.4, \"ㅎㅎㅎㅎ\": 0.6,\n",
    "    \"떡상\": 2.5, \"폭등\": 2.5,\n",
    "    \"떡락\": -2.5, \"폭락\": -2.5,\n",
    "    \"손절\": -1.5, \"망했다\": -3.0, \"조졌다\": -3.0,\n",
    "    \"고점\": -2.0,\n",
    "}\n",
    "final_lex = {**lex_score, **custom_score}\n",
    "\n",
    "\n",
    "# ====== (추가) HF 3분류 모델 로드 ======\n",
    "MODEL_NAME = \"jbeno/electra-base-classifier-sentiment\"\n",
    "clf = pipeline(\"text-classification\", model=MODEL_NAME, top_k=None, truncation=True)  # 3라벨 확률 [web:129][web:119]\n",
    "\n",
    "\n",
    "def scores_to_dict(out):\n",
    "    # out: [{'label':'negative','score':...}, ...]\n",
    "    d = {x[\"label\"].lower(): float(x[\"score\"]) for x in out}\n",
    "    # 모델마다 라벨 대소문자/형태 차이가 있을 수 있어 방어적으로 처리\n",
    "    return {\n",
    "        \"negative\": d.get(\"negative\", 0.0),\n",
    "        \"neutral\": d.get(\"neutral\", 0.0),\n",
    "        \"positive\": d.get(\"positive\", 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "def label_from_probs(p):\n",
    "    # p: {'negative':..., 'neutral':..., 'positive':...}\n",
    "    best = max(p, key=p.get)\n",
    "    return {\"negative\": \"부정\", \"neutral\": \"중립\", \"positive\": \"긍정\"}[best]\n",
    "\n",
    "\n",
    "def like_weight(like):\n",
    "    # 0~큰 값까지 안정적으로: 1 + log(1+like)\n",
    "    like = 0 if like is None else int(like)\n",
    "    return 1.0 + math.log1p(max(like, 0))\n",
    "\n",
    "\n",
    "# === 하루치 집계 ===\n",
    "token_counter = Counter()\n",
    "post_count = 0\n",
    "comment_count = 0\n",
    "\n",
    "# (추가) AI 집계용\n",
    "ai_weight_sum = 0.0\n",
    "ai_sum = {\"negative\": 0.0, \"neutral\": 0.0, \"positive\": 0.0}\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        post = json.loads(line)\n",
    "\n",
    "        if post.get(\"date\") != TARGET_DATE:\n",
    "            continue\n",
    "\n",
    "        post_count += 1\n",
    "        comments = post.get(\"comments\", [])\n",
    "        comment_count += len(comments)\n",
    "\n",
    "        # 1) 댓글 토큰화 → Counter 누적 (기존)\n",
    "        for c in comments:\n",
    "            text = normalize_repeats(c.get(\"comment\", \"\"))\n",
    "\n",
    "            tokens = [t.form for t in kiwi.tokenize(text) if t.tag not in STOP_TAGS]\n",
    "            token_counter.update(tokens)\n",
    "\n",
    "        # 2) (추가) 댓글을 AI로 감성분석 후 like 가중 합산\n",
    "        comment_texts = [normalize_repeats(c.get(\"comment\", \"\")) for c in comments if c.get(\"comment\")]\n",
    "        if comment_texts:\n",
    "            # batch 추론: 리스트로 넣으면 배치 처리됨 [web:99]\n",
    "            outs = clf(comment_texts, top_k=None)  # 버전에 따라 중첩 리스트/단일 리스트 형태가 달라질 수 있음 [web:106][web:129]\n",
    "\n",
    "            # outs 정규화: comment_texts 길이만큼 '각 댓글의 out(list[dict])'로 맞추기\n",
    "            if outs and isinstance(outs[0], dict):\n",
    "                # 텍스트 1개만 들어갔을 때 dict 리스트로 오는 케이스 방어\n",
    "                outs = [outs]\n",
    "\n",
    "            for c, out in zip(comments, outs):\n",
    "                p = scores_to_dict(out)\n",
    "                w = like_weight(c.get(\"like\", 0))\n",
    "                ai_sum[\"negative\"] += w * p[\"negative\"]\n",
    "                ai_sum[\"neutral\"] += w * p[\"neutral\"]\n",
    "                ai_sum[\"positive\"] += w * p[\"positive\"]\n",
    "                ai_weight_sum += w\n",
    "\n",
    "\n",
    "# 3) (기존) 하루치 사전 감성 점수\n",
    "matched_vocab = {w: final_lex[w] for w in token_counter if w in final_lex and float(final_lex[w]) != 0}\n",
    "day_sent_score = sum(token_counter[w] * float(matched_vocab[w]) for w in matched_vocab)\n",
    "lex_label = \"긍정\" if day_sent_score > 0 else \"부정\" if day_sent_score < 0 else \"중립\"\n",
    "\n",
    "# 4) (추가) 하루치 AI 확률/라벨\n",
    "if ai_weight_sum > 0:\n",
    "    ai_probs = {k: v / ai_weight_sum for k, v in ai_sum.items()}\n",
    "    ai_label = label_from_probs(ai_probs)\n",
    "else:\n",
    "    ai_probs = {\"negative\": 0.0, \"neutral\": 0.0, \"positive\": 0.0}\n",
    "    ai_label = \"중립\"\n",
    "\n",
    "# 5) (옵션) 하이브리드: AI가 애매할 때만 사전으로 보정(원하면 임계값 조정)\n",
    "ai_conf = max(ai_probs.values())\n",
    "if ai_conf < 0.50:\n",
    "    final_label = lex_label\n",
    "else:\n",
    "    final_label = ai_label\n",
    "\n",
    "\n",
    "print(\"날짜:\", TARGET_DATE)\n",
    "print(\"게시글 수:\", post_count)\n",
    "print(\"댓글 수:\", comment_count)\n",
    "print(\"유니크 토큰 수:\", len(token_counter))\n",
    "print(\"상위 토큰 20개:\", token_counter.most_common(20))\n",
    "print(\"사전 매칭 유니크 단어 수:\", len(matched_vocab))\n",
    "print(\"사전 감성 점수(빈도 가중):\", day_sent_score)\n",
    "print(\"사전 판정:\", lex_label)\n",
    "\n",
    "print(\"\\n[AI 모델]\", MODEL_NAME)\n",
    "print(\"AI 확률(가중평균):\", ai_probs)\n",
    "print(\"AI 판정:\", ai_label)\n",
    "print(\"최종(하이브리드) 판정:\", final_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfc313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U \"transformers[torch]\" huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ef27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "import pandas as pd\n",
    "\n",
    "jsonl_path = r\"..\\data\\fmkorea_samsung_hot_posts.jsonl\"\n",
    "\n",
    "START_DATE = pd.to_datetime(\"2026-01-01\").date()\n",
    "END_DATE   = pd.to_datetime(\"2026-01-16\").date()\n",
    "\n",
    "OUT_CSV = r\"..\\완료\\daily_outputs\\fmkorea_tokens_daily_2025-01-14_2026-01-14.csv\"\n",
    "\n",
    "\n",
    "def normalize_repeats(s: str) -> str:\n",
    "    s = re.sub(r\"ㅋ{5,}\", \"ㅋㅋㅋㅋ\", s)\n",
    "    s = re.sub(r\"ㅎ{5,}\", \"ㅎㅎㅎㅎ\", s)\n",
    "    s = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", s)\n",
    "    s = re.sub(r\"ㅜ{3,}\", \"ㅜㅜ\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Kiwi\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word(\"삼전\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"삼성전자\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하닉\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하이닉스\", \"NNP\", 0)\n",
    "\n",
    "STOP_TAGS = {\n",
    "    \"JKS\",\"JKC\",\"JKG\",\"JKO\",\"JKB\",\"JKV\",\"JKQ\",\"JX\",\"JC\",\n",
    "    \"EP\",\"EF\",\"EC\",\"ETN\",\"ETM\",\n",
    "    \"SF\",\"SP\",\"SS\",\"SE\",\"SO\",\"SW\"\n",
    "}\n",
    "\n",
    "def tokenize_text(text: str) -> list[str]:\n",
    "    \"\"\"본문/댓글 공통 토큰화: 반복 문자 정규화 + 품사 필터\"\"\"\n",
    "    text = normalize_repeats(text or \"\")\n",
    "    return [t.form for t in kiwi.tokenize(text) if t.tag not in STOP_TAGS]  # kiwi.tokenize 사용 [web:73]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 날짜별 누적 저장소\n",
    "# =========================\n",
    "daily_post_count = defaultdict(int)\n",
    "daily_comment_count = defaultdict(int)\n",
    "daily_token_counter = defaultdict(Counter)\n",
    "\n",
    "# (선택) 너무 커지면 끄기: 날짜별 상위 토큰만 뽑아 저장할 때 유용\n",
    "# daily_total_token = defaultdict(int)\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        post = json.loads(line)\n",
    "\n",
    "        # 1) 날짜 파싱/필터\n",
    "        date_str = post.get(\"date\")\n",
    "        if not date_str:\n",
    "            continue\n",
    "\n",
    "        d = pd.to_datetime(date_str, errors=\"coerce\")\n",
    "        if pd.isna(d):\n",
    "            continue\n",
    "\n",
    "        d = d.date()\n",
    "        if d < START_DATE or d > END_DATE:\n",
    "            continue\n",
    "\n",
    "        # 2) 본문 + 댓글 토큰화\n",
    "        daily_post_count[d] += 1\n",
    "\n",
    "        # ✅ 본문 키 이름은 데이터에 맞게 조정 가능: content/body/text 중 하나일 수 있음\n",
    "        body = post.get(\"content\") or post.get(\"body\") or post.get(\"text\") or \"\"\n",
    "        daily_token_counter[d].update(tokenize_text(body))\n",
    "\n",
    "        comments = post.get(\"comments\", [])\n",
    "        daily_comment_count[d] += len(comments)\n",
    "\n",
    "        for c in comments:\n",
    "            # 네 원본 코드: c.get(\"comment\")\n",
    "            c_text = \"\"\n",
    "            if isinstance(c, dict):\n",
    "                c_text = c.get(\"comment\", \"\") or c.get(\"content\", \"\") or c.get(\"text\", \"\")\n",
    "            else:\n",
    "                c_text = str(c)\n",
    "            daily_token_counter[d].update(tokenize_text(c_text))\n",
    "\n",
    "# =========================\n",
    "# 일별 DataFrame 생성(빈 날짜 0 포함)\n",
    "# =========================\n",
    "rows = []\n",
    "for d in pd.date_range(START_DATE, END_DATE, freq=\"D\").date:\n",
    "    counter = daily_token_counter.get(d, Counter())\n",
    "    total_tokens = sum(counter.values())\n",
    "    unique_tokens = len(counter)\n",
    "\n",
    "    rows.append({\n",
    "        \"날짜\": pd.to_datetime(d).strftime(\"%Y-%m-%d\"),\n",
    "        \"게시글수\": int(daily_post_count.get(d, 0)),\n",
    "        \"댓글수\": int(daily_comment_count.get(d, 0)),\n",
    "        \"총토큰수\": int(total_tokens),\n",
    "        \"유니크토큰수\": int(unique_tokens),\n",
    "        \"상위토큰20\": counter.most_common(20),  # most_common 사용 [web:386]\n",
    "    })\n",
    "\n",
    "daily_df = pd.DataFrame(rows)\n",
    "daily_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[저장]\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from kiwipiepy import Kiwi\n",
    "import pandas as pd\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "jsonl_path = r\"..\\data\\fmkorea_samsung_hot_posts.jsonl\"\n",
    "\n",
    "START_DATE = pd.to_datetime(\"2025-12-05\").date()\n",
    "END_DATE   = pd.to_datetime(\"2025-12-05\").date()\n",
    "\n",
    "OUT_CSV = r\"..\\완료\\daily_outputs\\fmkorea_tokens.csv\"\n",
    "OUT_WC_PNG = r\"..\\완료\\daily_outputs\\wc.png\"\n",
    "FONT_PATH = r\"C:\\Windows\\Fonts\\malgun.ttf\"\n",
    "\n",
    "\n",
    "def normalize_repeats(s: str) -> str:\n",
    "    s = re.sub(r\"ㅋ{5,}\", \"ㅋㅋㅋㅋ\", s)\n",
    "    s = re.sub(r\"ㅎ{5,}\", \"ㅎㅎㅎㅎ\", s)\n",
    "    s = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", s)\n",
    "    s = re.sub(r\"ㅜ{3,}\", \"ㅜㅜ\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Kiwi\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word(\"삼전\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"삼성전자\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하닉\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하이닉스\", \"NNP\", 0)\n",
    "\n",
    "STOP_TAGS = {\n",
    "    \"JKS\",\"JKC\",\"JKG\",\"JKO\",\"JKB\",\"JKV\",\"JKQ\",\"JX\",\"JC\",\n",
    "    \"EP\",\"EF\",\"EC\",\"ETN\",\"ETM\",\n",
    "    \"SF\",\"SP\",\"SS\",\"SE\",\"SO\",\"SW\"\n",
    "}\n",
    "\n",
    "def tokenize_text(text: str) -> list[str]:\n",
    "    \"\"\"방법2: Kiwi 토큰 + 1글자 무시\"\"\"\n",
    "    text = normalize_repeats(text or \"\")\n",
    "    out = []\n",
    "    for t in kiwi.tokenize(text):  # t.form / t.tag 사용 [web:73]\n",
    "        if t.tag in STOP_TAGS:\n",
    "            continue\n",
    "        if len(t.form) < 2:        # ✅ 1글자 토큰 제거\n",
    "            continue\n",
    "        out.append(t.form)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 날짜별 누적 저장소\n",
    "# =========================\n",
    "daily_post_count = defaultdict(int)\n",
    "daily_comment_count = defaultdict(int)\n",
    "daily_token_counter = defaultdict(Counter)\n",
    "\n",
    "# 기간 전체 토큰(워드클라우드 1장용)\n",
    "period_counter = Counter()\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        post = json.loads(line)\n",
    "\n",
    "        date_str = post.get(\"date\")\n",
    "        if not date_str:\n",
    "            continue\n",
    "\n",
    "        d = pd.to_datetime(date_str, errors=\"coerce\")\n",
    "        if pd.isna(d):\n",
    "            continue\n",
    "\n",
    "        d = d.date()\n",
    "        if d < START_DATE or d > END_DATE:\n",
    "            continue\n",
    "\n",
    "        daily_post_count[d] += 1\n",
    "\n",
    "        body = post.get(\"content\") or post.get(\"body\") or post.get(\"text\") or \"\"\n",
    "        toks = tokenize_text(body)\n",
    "        daily_token_counter[d].update(toks)\n",
    "        period_counter.update(toks)\n",
    "\n",
    "        comments = post.get(\"comments\", [])\n",
    "        daily_comment_count[d] += len(comments)\n",
    "\n",
    "        for c in comments:\n",
    "            if isinstance(c, dict):\n",
    "                c_text = c.get(\"comment\", \"\") or c.get(\"content\", \"\") or c.get(\"text\", \"\")\n",
    "            else:\n",
    "                c_text = str(c)\n",
    "            toks = tokenize_text(c_text)\n",
    "            daily_token_counter[d].update(toks)\n",
    "            period_counter.update(toks)\n",
    "\n",
    "# =========================\n",
    "# 일별 DataFrame 생성\n",
    "# =========================\n",
    "rows = []\n",
    "for d in pd.date_range(START_DATE, END_DATE, freq=\"D\").date:\n",
    "    counter = daily_token_counter.get(d, Counter())\n",
    "    total_tokens = sum(counter.values())\n",
    "    unique_tokens = len(counter)\n",
    "\n",
    "    rows.append({\n",
    "        \"날짜\": pd.to_datetime(d).strftime(\"%Y-%m-%d\"),\n",
    "        \"게시글수\": int(daily_post_count.get(d, 0)),\n",
    "        \"댓글수\": int(daily_comment_count.get(d, 0)),\n",
    "        \"총토큰수\": int(total_tokens),\n",
    "        \"유니크토큰수\": int(unique_tokens),\n",
    "        \"상위토큰20\": counter.most_common(20),\n",
    "    })\n",
    "\n",
    "daily_df = pd.DataFrame(rows)\n",
    "daily_df.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[CSV 저장]\", OUT_CSV)\n",
    "\n",
    "# =========================\n",
    "# 기간 전체 워드클라우드 1장 생성\n",
    "# =========================\n",
    "wc = WordCloud(\n",
    "    font_path=FONT_PATH,\n",
    "    background_color=\"white\",\n",
    "    width=1400, height=700,\n",
    "    max_words=200\n",
    ").generate_from_frequencies(dict(period_counter))  # 빈도 기반 생성 [web:584][web:592]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_WC_PNG, dpi=200)\n",
    "plt.show()\n",
    "print(\"[워드클라우드 저장]\", OUT_WC_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed84d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df066f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install electra-classifier transformers torch kiwipiepy pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c751119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip uninstall -y torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a47c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "DEVICE: cuda\n",
      "Model device: cuda:0\n",
      "[PASS1 START] building lexicon ...\n",
      "[PASS1] scored_comments=1,000 | lines=487 | posts_seen=487 | 42.41 comments/sec | elapsed=0.4 min\n",
      "[PASS1] scored_comments=2,000 | lines=533 | posts_seen=533 | 43.26 comments/sec | elapsed=0.8 min\n",
      "[PASS1] scored_comments=3,000 | lines=582 | posts_seen=582 | 41.64 comments/sec | elapsed=1.2 min\n",
      "[PASS1 DONE] final ckpt saved: ..\\output\\ckpt_pass1.pkl\n",
      "[SAVE] ..\\output\\auto_fg_lexicon.csv | rows: 161\n",
      "[PASS2 START] building daily index ...\n",
      "[PASS2] processed_comments=1,000 | lines=499 | posts_seen=499 | 659.89 comments/sec | elapsed=0.0 min\n",
      "[PASS2] processed_comments=2,000 | lines=564 | posts_seen=564 | 830.82 comments/sec | elapsed=0.0 min\n",
      "[PASS2 DONE] final ckpt saved: ..\\output\\ckpt_pass2.pkl\n",
      "[SAVE] ..\\output\\daily_fg_index.csv\n",
      "MODEL: jbeno/electra-base-classifier-sentiment | DEVICE: cuda\n",
      "SCALE_DAILY: 8.0 | CENTER: 0.55\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from kiwipiepy import Kiwi\n",
    "from transformers import AutoTokenizer\n",
    "from electra_classifier import ElectraClassifier\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 설정\n",
    "# =========================\n",
    "JSONL_PATH = r\"..\\data\\fmkorea_samsung_hot_posts.jsonl\"\n",
    "\n",
    "# 11/05 ~ 11/06만\n",
    "START_DATE = \"2025-11-05\"\n",
    "END_DATE   = \"2025-12-06\"\n",
    "\n",
    "OUT_DIR = r\"..\\output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_LEXICON_CSV = os.path.join(OUT_DIR, \"auto_fg_lexicon.csv\")\n",
    "OUT_DAILY_CSV   = os.path.join(OUT_DIR, \"daily_fg_index.csv\")\n",
    "\n",
    "CKPT_PASS1 = os.path.join(OUT_DIR, \"ckpt_pass1.pkl\")\n",
    "CKPT_PASS2 = os.path.join(OUT_DIR, \"ckpt_pass2.pkl\")\n",
    "\n",
    "MODEL_NAME = \"jbeno/electra-base-classifier-sentiment\"\n",
    "\n",
    "PRINT_EVERY = 1000   # 진행 로그 (댓글 기준)\n",
    "CKPT_EVERY  = 5000   # 체크포인트 저장 (댓글 기준)\n",
    "\n",
    "# 민감도\n",
    "SCALE_DAILY = 8.0\n",
    "\n",
    "# (빠른 보정) 포화 방지/부호 만들기용 센터\n",
    "CENTER = 0.55\n",
    "\n",
    "MIN_COUNT = 20\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 모델/토크나이저 로드\n",
    "# =========================\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = ElectraClassifier.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "id2label = {int(k): str(v).lower() for k, v in model.config.id2label.items()}\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 전처리 / Kiwi 토큰화\n",
    "# =========================\n",
    "def normalize_repeats(s: str) -> str:\n",
    "    s = s or \"\"\n",
    "    s = re.sub(r\"ㅋ{5,}\", \"ㅋㅋㅋㅋ\", s)\n",
    "    s = re.sub(r\"ㅎ{5,}\", \"ㅎㅎㅎㅎ\", s)\n",
    "    s = re.sub(r\"ㅠ{3,}\", \"ㅠㅠ\", s)\n",
    "    s = re.sub(r\"ㅜ{3,}\", \"ㅜㅜ\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "kiwi = Kiwi()\n",
    "kiwi.add_user_word(\"삼전\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"삼성전자\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하닉\", \"NNP\", 0)\n",
    "kiwi.add_user_word(\"하이닉스\", \"NNP\", 0)\n",
    "\n",
    "STOP_TAGS = {\n",
    "    \"JKS\",\"JKC\",\"JKG\",\"JKO\",\"JKB\",\"JKV\",\"JKQ\",\"JX\",\"JC\",\n",
    "    \"EP\",\"EF\",\"EC\",\"ETN\",\"ETM\",\n",
    "    \"SF\",\"SP\",\"SS\",\"SE\",\"SO\",\"SW\"\n",
    "}\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    text = normalize_repeats(text)\n",
    "    out = []\n",
    "    for t in kiwi.tokenize(text):\n",
    "        if t.tag in STOP_TAGS:\n",
    "            continue\n",
    "        if len(t.form) < 2:\n",
    "            continue\n",
    "        out.append(t.form)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# AI 추론 (원문 -> 확률 dict)\n",
    "# =========================\n",
    "def run_clf_probs(text: str) -> dict:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)\n",
    "    probs = torch.softmax(logits, dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    out = {id2label[i]: float(probs[i]) for i in range(len(probs))}\n",
    "    return {\n",
    "        \"negative\": out.get(\"negative\", 0.0),\n",
    "        \"neutral\": out.get(\"neutral\", 0.0),\n",
    "        \"positive\": out.get(\"positive\", 0.0),\n",
    "    }\n",
    "\n",
    "\n",
    "# 부정은 확실히 -, neutral 영향 제거(극성만 사용)\n",
    "def fg_sentence_score(probs: dict) -> float:\n",
    "    p = probs[\"positive\"]\n",
    "    n = probs[\"negative\"]\n",
    "    denom = p + n\n",
    "    if denom < 1e-8:\n",
    "        return 0.0\n",
    "    return (p - n) / denom   # [-1, 1]\n",
    "\n",
    "\n",
    "def like_weight(like):\n",
    "    like = 0 if like is None else int(like)\n",
    "    return 1.0 + math.log1p(max(like, 0))\n",
    "\n",
    "\n",
    "def post_weight(views, votes):\n",
    "    views = 0 if views is None else int(views)\n",
    "    votes = 0 if votes is None else int(votes)\n",
    "    return 1.0 + math.log1p(max(views, 0)) + 0.5 * math.log1p(max(votes, 0))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 기간 설정\n",
    "# =========================\n",
    "start_d = pd.to_datetime(START_DATE).date()\n",
    "end_d   = pd.to_datetime(END_DATE).date()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 체크포인트 유틸\n",
    "# =========================\n",
    "def save_ckpt_pass1(state: dict):\n",
    "    tmp = CKPT_PASS1 + \".tmp\"\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    os.replace(tmp, CKPT_PASS1)\n",
    "\n",
    "def load_ckpt_pass1():\n",
    "    if not os.path.exists(CKPT_PASS1):\n",
    "        return None\n",
    "    with open(CKPT_PASS1, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_ckpt_pass2(state: dict):\n",
    "    tmp = CKPT_PASS2 + \".tmp\"\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    os.replace(tmp, CKPT_PASS2)\n",
    "\n",
    "def load_ckpt_pass2():\n",
    "    if not os.path.exists(CKPT_PASS2):\n",
    "        return None\n",
    "    with open(CKPT_PASS2, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PASS1) 단어 점수 역추정\n",
    "# =========================\n",
    "ckpt1 = load_ckpt_pass1()\n",
    "if ckpt1:\n",
    "    print(\"[PASS1 RESUME] load:\", CKPT_PASS1)\n",
    "    word_score_wsum = defaultdict(float, ckpt1[\"word_score_wsum\"])\n",
    "    word_weight_sum = defaultdict(float, ckpt1[\"word_weight_sum\"])\n",
    "    word_count      = defaultdict(int,   ckpt1[\"word_count\"])\n",
    "    resume_line_idx = int(ckpt1[\"line_idx\"])\n",
    "    scored_comments = int(ckpt1[\"scored_comments\"])\n",
    "else:\n",
    "    word_score_wsum = defaultdict(float)\n",
    "    word_weight_sum = defaultdict(float)\n",
    "    word_count      = defaultdict(int)\n",
    "    resume_line_idx = 0\n",
    "    scored_comments = 0\n",
    "\n",
    "t0 = time.time()\n",
    "posts_seen = 0\n",
    "lines_seen = 0\n",
    "\n",
    "print(\"[PASS1 START] building lexicon ...\")\n",
    "\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line_idx, line in enumerate(f):\n",
    "        lines_seen = line_idx + 1\n",
    "        if line_idx < resume_line_idx:\n",
    "            continue\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        post = json.loads(line)\n",
    "        posts_seen += 1\n",
    "\n",
    "        d = pd.to_datetime(post.get(\"date\"), errors=\"coerce\")\n",
    "        if pd.isna(d):\n",
    "            continue\n",
    "        d = d.date()\n",
    "        if d < start_d or d > end_d:\n",
    "            continue\n",
    "\n",
    "        w_post = post_weight(post.get(\"views\", 0), post.get(\"votes\", 0))\n",
    "\n",
    "        comments = post.get(\"comments\", []) or []\n",
    "        for c in comments:\n",
    "            if not isinstance(c, dict):\n",
    "                continue\n",
    "\n",
    "            text = normalize_repeats(c.get(\"comment\", \"\") or \"\")\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            tokens = tokenize(text)\n",
    "            if not tokens:\n",
    "                continue\n",
    "\n",
    "            probs = run_clf_probs(text)\n",
    "            fg = fg_sentence_score(probs)\n",
    "\n",
    "            cnt = Counter(tokens)\n",
    "            w = like_weight(c.get(\"like\", 0)) + 0.2 * w_post\n",
    "\n",
    "            for wtok, n in cnt.items():\n",
    "                word_score_wsum[wtok] += fg * w * n\n",
    "                word_weight_sum[wtok] += w * n\n",
    "                word_count[wtok] += n\n",
    "\n",
    "            scored_comments += 1\n",
    "\n",
    "            if scored_comments % PRINT_EVERY == 0:\n",
    "                dt = time.time() - t0\n",
    "                speed = scored_comments / max(dt, 1e-9)\n",
    "                print(f\"[PASS1] scored_comments={scored_comments:,} | lines={lines_seen:,} | posts_seen={posts_seen:,} \"\n",
    "                      f\"| {speed:.2f} comments/sec | elapsed={dt/60:.1f} min\")\n",
    "\n",
    "            if scored_comments % CKPT_EVERY == 0:\n",
    "                state = {\n",
    "                    \"line_idx\": lines_seen,\n",
    "                    \"scored_comments\": scored_comments,\n",
    "                    \"word_score_wsum\": dict(word_score_wsum),\n",
    "                    \"word_weight_sum\": dict(word_weight_sum),\n",
    "                    \"word_count\": dict(word_count),\n",
    "                }\n",
    "                save_ckpt_pass1(state)\n",
    "                print(f\"[PASS1 CKPT SAVED] {CKPT_PASS1} (scored_comments={scored_comments:,}, line_idx={lines_seen:,})\")\n",
    "\n",
    "state = {\n",
    "    \"line_idx\": lines_seen,\n",
    "    \"scored_comments\": scored_comments,\n",
    "    \"word_score_wsum\": dict(word_score_wsum),\n",
    "    \"word_weight_sum\": dict(word_weight_sum),\n",
    "    \"word_count\": dict(word_count),\n",
    "}\n",
    "save_ckpt_pass1(state)\n",
    "print(f\"[PASS1 DONE] final ckpt saved: {CKPT_PASS1}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PASS1 결과 저장\n",
    "# =========================\n",
    "lex_rows = []\n",
    "for wtok in word_score_wsum.keys():\n",
    "    if word_weight_sum[wtok] <= 0:\n",
    "        continue\n",
    "    score = word_score_wsum[wtok] / word_weight_sum[wtok]\n",
    "    lex_rows.append({\n",
    "        \"token\": wtok,\n",
    "        \"fg_word_score\": float(score),\n",
    "        \"count\": int(word_count[wtok]),\n",
    "        \"weight_sum\": float(word_weight_sum[wtok]),\n",
    "    })\n",
    "\n",
    "lex_df = pd.DataFrame(lex_rows)\n",
    "lex_df = lex_df[lex_df[\"count\"] >= MIN_COUNT].copy()\n",
    "\n",
    "# 점수 기준 정렬(낮은 점수(부정 쪽)부터 보기)\n",
    "lex_df = lex_df.sort_values([\"fg_word_score\", \"count\"], ascending=[True, False])\n",
    "\n",
    "lex_df.to_csv(OUT_LEXICON_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[SAVE]\", OUT_LEXICON_CSV, \"| rows:\", len(lex_df))\n",
    "\n",
    "lex_map = dict(zip(lex_df[\"token\"], lex_df[\"fg_word_score\"]))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PASS2) 날짜별 FG (11/05~11/06)\n",
    "# =========================\n",
    "ckpt2 = load_ckpt_pass2()\n",
    "if ckpt2:\n",
    "    print(\"[PASS2 RESUME] load:\", CKPT_PASS2)\n",
    "    daily_num = defaultdict(float, ckpt2[\"daily_num\"])\n",
    "    daily_den = defaultdict(float, ckpt2[\"daily_den\"])\n",
    "    resume_line_idx2 = int(ckpt2[\"line_idx\"])\n",
    "    processed_comments2 = int(ckpt2[\"processed_comments\"])\n",
    "else:\n",
    "    daily_num = defaultdict(float)\n",
    "    daily_den = defaultdict(float)\n",
    "    resume_line_idx2 = 0\n",
    "    processed_comments2 = 0\n",
    "\n",
    "t0 = time.time()\n",
    "lines_seen = 0\n",
    "posts_seen = 0\n",
    "print(\"[PASS2 START] building daily index ...\")\n",
    "\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line_idx, line in enumerate(f):\n",
    "        lines_seen = line_idx + 1\n",
    "        if line_idx < resume_line_idx2:\n",
    "            continue\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        post = json.loads(line)\n",
    "        posts_seen += 1\n",
    "\n",
    "        d = pd.to_datetime(post.get(\"date\"), errors=\"coerce\")\n",
    "        if pd.isna(d):\n",
    "            continue\n",
    "        d = d.date()\n",
    "        if d < start_d or d > end_d:\n",
    "            continue\n",
    "\n",
    "        w_post = post_weight(post.get(\"views\", 0), post.get(\"votes\", 0))\n",
    "\n",
    "        comments = post.get(\"comments\", []) or []\n",
    "        for c in comments:\n",
    "            if not isinstance(c, dict):\n",
    "                continue\n",
    "\n",
    "            text = normalize_repeats(c.get(\"comment\", \"\") or \"\")\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            tokens = tokenize(text)\n",
    "            if not tokens:\n",
    "                continue\n",
    "\n",
    "            cnt = Counter(tokens)\n",
    "\n",
    "            num = 0.0\n",
    "            den = 0.0\n",
    "            for wtok, n in cnt.items():\n",
    "                s = lex_map.get(wtok)\n",
    "                if s is None:\n",
    "                    continue\n",
    "                num += s * n\n",
    "                den += n\n",
    "            if den == 0:\n",
    "                continue\n",
    "\n",
    "            # 사전 기반 코멘트 점수\n",
    "            comment_fg = num / den\n",
    "\n",
    "            # (빠른 보정) 포화 방지용 센터링\n",
    "            comment_fg = comment_fg - CENTER\n",
    "\n",
    "            # 민감하게 확대(항상 -1~1 유지)\n",
    "            comment_fg = math.tanh(SCALE_DAILY * comment_fg)\n",
    "\n",
    "            w = like_weight(c.get(\"like\", 0)) + 0.2 * w_post\n",
    "            daily_num[d] += comment_fg * w\n",
    "            daily_den[d] += w\n",
    "\n",
    "            processed_comments2 += 1\n",
    "\n",
    "            if processed_comments2 % PRINT_EVERY == 0:\n",
    "                dt = time.time() - t0\n",
    "                speed = processed_comments2 / max(dt, 1e-9)\n",
    "                print(f\"[PASS2] processed_comments={processed_comments2:,} | lines={lines_seen:,} | posts_seen={posts_seen:,} \"\n",
    "                      f\"| {speed:.2f} comments/sec | elapsed={dt/60:.1f} min\")\n",
    "\n",
    "            if processed_comments2 % CKPT_EVERY == 0:\n",
    "                state2 = {\n",
    "                    \"line_idx\": lines_seen,\n",
    "                    \"processed_comments\": processed_comments2,\n",
    "                    \"daily_num\": dict(daily_num),\n",
    "                    \"daily_den\": dict(daily_den),\n",
    "                }\n",
    "                save_ckpt_pass2(state2)\n",
    "                print(f\"[PASS2 CKPT SAVED] {CKPT_PASS2} (processed_comments={processed_comments2:,}, line_idx={lines_seen:,})\")\n",
    "\n",
    "state2 = {\n",
    "    \"line_idx\": lines_seen,\n",
    "    \"processed_comments\": processed_comments2,\n",
    "    \"daily_num\": dict(daily_num),\n",
    "    \"daily_den\": dict(daily_den),\n",
    "}\n",
    "save_ckpt_pass2(state2)\n",
    "print(f\"[PASS2 DONE] final ckpt saved: {CKPT_PASS2}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 결과 저장 (0인 날은 50 유지)\n",
    "# =========================\n",
    "rows = []\n",
    "for d in pd.date_range(start_d, end_d, freq=\"D\").date:\n",
    "    fg = 0.0 if daily_den[d] == 0 else daily_num[d] / daily_den[d]\n",
    "    rows.append({\n",
    "        \"date\": pd.to_datetime(d).strftime(\"%Y-%m-%d\"),\n",
    "        \"fg_minus1_to_1\": float(fg),\n",
    "        \"fg_0_100\": float((fg + 1.0) * 50.0),\n",
    "        \"weight_sum\": float(daily_den[d]),\n",
    "    })\n",
    "\n",
    "daily_df = pd.DataFrame(rows)\n",
    "daily_df.to_csv(OUT_DAILY_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[SAVE]\", OUT_DAILY_CSV)\n",
    "print(\"MODEL:\", MODEL_NAME, \"| DEVICE:\", DEVICE)\n",
    "print(\"SCALE_DAILY:\", SCALE_DAILY, \"| CENTER:\", CENTER)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
