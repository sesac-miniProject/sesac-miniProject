{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7443c7f",
   "metadata": {},
   "source": [
    "모듈 다운로드.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89303fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad9d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc62568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903072a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"huggingface_hub[hf_xet]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2caffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"GPU 사용 가능 여부: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU를 찾을 수 없어 CPU를 사용합니다.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ceca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"snunlp/KR-FinBert-SC\"\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 모델 로드 및 GPU 할당\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "model.to(device)  # 이 코드가 모델을 GPU로 보냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a422d4",
   "metadata": {},
   "source": [
    "# 모델 추가학습 코드\n",
    "\n",
    "FMKorea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2848ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 중인 장치: cuda\n",
      "데이터셋 준비 중...\n",
      "토크나이징 중...\n",
      "모델 로드 중...\n",
      "학습 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3363' max='3363' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3363/3363 02:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.273100</td>\n",
       "      <td>0.184250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.149888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.149835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료: ./finetuned_stock_bert\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# 1. 설정 및 장치 확인\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"현재 사용 중인 장치: {device}\")\n",
    "\n",
    "# 2. 리스트 (생략 없이 그대로 사용)\n",
    "# 업데이트된 Fear 리스트\n",
    "fear_words = [\n",
    "    '공포', '하락', '인버스', '패닉', '셀링', '손절', '풀숏', '숏', '불안', '위험', '회피', \n",
    "    '리스크', '침체', '변동성', '마이너스', '물타기', '패닉셀', '매도', '존버', '익절', \n",
    "    '조정', '도망', '포기', '이탈', '환장', '한강', '자살', '수온', '음전', '설거지', \n",
    "    '개미털이', '공매도', '떡락', '탈주', '금리인상', '추락', '피바다', '반등없음', '쫄림', \n",
    "    '절망', '멘붕', '현금확보', '지옥', '외인매도', '기관매도', '양아치', '사기', '물림', \n",
    "    '손실확정', '악재', '청산', '잡주', '빠지', '망하', '좆', '새끼', '죽', '반대', \n",
    "    '힘들', '조심', '급락', '나쁘', '당하', '병신', '버블', '탈출', '끝물', '구조대', \n",
    "    '물리', '관세', '밀리', '내리', '떨어지', '음봉', '하향', '부진', '부담', '우려', \n",
    "    '실망', '비싸', '관망', '박스권', '고평가', '거품',\n",
    "    '팔', '지랄', '좃', '던지', '버스', '뒤지', '처박', '실패', '시발', '무섭', '공포장'\n",
    "]\n",
    "greed_words = [\n",
    "    '탐욕', '레버리지', '풀롱', '롱', '수출호재', '호재', '기대', '떡상', '드가자', '가즈아', \n",
    "    '매수', '신고가', '고점', '몰빵', '상승', '폭등', '불장', '랠리', 'FOMO', '포모', \n",
    "    '효자', '불타기', '과열', '쭉쭉', '급등주', '급등', '추매', '최고', '대장', '사랑', \n",
    "    '감사', '갓전자', '갓하이닉스', '갓현대', '갓차', '개미승리', '상방확정', '호재반영', \n",
    "    '롱진입', '외인매수', '기관매수', '마진확대', '우상향', '폭주기관차', '상승장', '행복', \n",
    "    '안착', '성공', '부럽', '커피값', '저녁값', '소고기', '광기', '회복', '영끌', '빚투', \n",
    "    '축하', '와우', '대박', '추격매수', '전고점', '매집중', '돌파', '수익', '반등', '양봉', \n",
    "    '실현', '베팅', '홀딩', '기대감', '차익', '부자', '강하', '수혜', '강세', '성장', \n",
    "    '사이클', '수급',\n",
    "    '오르', '이익', '기회', '모으', '수주', '상방', '가치', '수익중'\n",
    "]\n",
    "\n",
    "# 3. 데이터 로드 및 라벨링 함수\n",
    "def prepare_dataset(jsonl_path):\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        raise FileNotFoundError(f\"파일을 찾을 수 없습니다: {jsonl_path}\")\n",
    "    \n",
    "    data = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            post = json.loads(line)\n",
    "            texts = [post.get('title', ''), post.get('content', '')]\n",
    "            for c in post.get('comments', []) or []:\n",
    "                if isinstance(c, dict): texts.append(c.get('comment', ''))\n",
    "            \n",
    "            for text in texts:\n",
    "                text = str(text).strip()\n",
    "                if len(text) < 5: continue\n",
    "                f_score = sum(1 for w in fear_words if w in text)\n",
    "                g_score = sum(1 for w in greed_words if w in text)\n",
    "                label = 0 if f_score > g_score else (2 if g_score > f_score else 1)\n",
    "                data.append({'text': text, 'label': label})\n",
    "    return pd.DataFrame(data).drop_duplicates()\n",
    "\n",
    "# 4. 데이터셋 클래스 정의\n",
    "class StockDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 5. 실행 로직 (이 순서가 중요합니다)\n",
    "print(\"데이터셋 준비 중...\")\n",
    "df = prepare_dataset('../data/fmkorea_hynix_hot_posts.jsonl')\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, stratify=df['label'], random_state=42)\n",
    "\n",
    "MODEL_NAME = \"snunlp/KR-FinBert-SC\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"토크나이징 중...\")\n",
    "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# 변수 생성 확인\n",
    "train_dataset = StockDataset(train_encodings, train_df['label'].tolist())\n",
    "val_dataset = StockDataset(val_encodings, val_df['label'].tolist())\n",
    "\n",
    "# 6. 모델 학습\n",
    "print(\"모델 로드 중...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./stock_model_checkpoints',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset, # 이제 메모리에 확실히 존재함\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"학습 시작...\")\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "model.save_pretrained(\"./finetuned_stock_bert\")\n",
    "tokenizer.save_pretrained(\"./finetuned_stock_bert\")\n",
    "print(\"모델 저장 완료: ./finetuned_stock_bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55cf5aa",
   "metadata": {},
   "source": [
    "## 추가학습된 모델로 데이터 공포/탐욕 지수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10dd1fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 시작 (장치: cuda)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [01:31<00:00, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[완료] 지수 산출 결과가 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# 1. 설정 및 경로\n",
    "# =========================\n",
    "JSONL_PATH = r\"..\\data\\fmkorea_hynix_hot_posts.jsonl\"\n",
    "OUT_DIR = r\"..\\output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_DAILY_CSV = os.path.join(OUT_DIR, \"daily_fng_balanced.csv\")\n",
    "OUT_WEEKLY_CSV = os.path.join(OUT_DIR, \"weekly_fng_balanced.csv\")\n",
    "\n",
    "START_DATE = \"2025-01-14\"\n",
    "END_DATE   = \"2026-01-14\"\n",
    "\n",
    "MODEL_NAME = \"./finetuned_stock_bert\" \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# =========================\n",
    "# 2. 모델 로드 및 추론 로직 (중립 문턱 최적화)\n",
    "# =========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def probs_to_fng(logits):\n",
    "    probs = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
    "    f_p, n_p, g_p = probs[0], probs[1], probs[2]\n",
    "    \n",
    "    # [조정] 중립 임계값을 0.60으로 살짝 상향 (0.5는 너무 예민하여 노이즈가 심함)\n",
    "    neutral_threshold = 0.60 \n",
    "    \n",
    "    if n_p > neutral_threshold:\n",
    "        label, score = \"neutral\", 0.0\n",
    "    elif f_p > g_p:\n",
    "        label, score = \"fear\", -1.0\n",
    "    else:\n",
    "        label, score = \"greed\", 1.0\n",
    "        \n",
    "    return label, score\n",
    "\n",
    "def infer_one(text: str):\n",
    "    if not text or not str(text).strip():\n",
    "        return None\n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, max_length=128).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    return probs_to_fng(logits)\n",
    "\n",
    "# =========================\n",
    "# 3. 데이터 집계\n",
    "# =========================\n",
    "start_d = pd.to_datetime(START_DATE).date()\n",
    "end_d   = pd.to_datetime(END_DATE).date()\n",
    "\n",
    "daily_stats = defaultdict(lambda: {\"fear\": 0, \"neutral\": 0, \"greed\": 0, \"total_w\": 0.0})\n",
    "\n",
    "print(f\"분석 시작 (장치: {DEVICE})...\")\n",
    "\n",
    "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        post = json.loads(line)\n",
    "        d = pd.to_datetime(post.get(\"date\"), errors=\"coerce\")\n",
    "        if pd.isna(d): continue\n",
    "        d_date = d.date()\n",
    "        if d_date < start_d or d_date > end_d: continue\n",
    "\n",
    "        # 제목 가중치를 1.5배로 약간 하향 (2.0은 변동성을 너무 키움)\n",
    "        texts = [('제목', post.get(\"title\", \"\")), ('본문', post.get(\"content\", \"\"))]\n",
    "        for c in post.get(\"comments\", []) or []:\n",
    "            if isinstance(c, dict):\n",
    "                texts.append(('댓글', c.get(\"comment\", \"\")))\n",
    "\n",
    "        for t_type, t in texts:\n",
    "            out = infer_one(t)\n",
    "            if out is None: continue\n",
    "            label, s = out\n",
    "            \n",
    "            weight = 1.5 if t_type == '제목' else 1.0\n",
    "            daily_stats[d_date][label] += weight\n",
    "            daily_stats[d_date][\"total_w\"] += weight\n",
    "\n",
    "# =========================\n",
    "# 4. 결과 산출 및 평활화 (Smoothing)\n",
    "# =========================\n",
    "def calculate_raw_index(stats):\n",
    "    f, n, g = stats[\"fear\"], stats[\"neutral\"], stats[\"greed\"]\n",
    "    tw = stats[\"total_w\"]\n",
    "    if tw == 0: return 50.0, 0.0\n",
    "    \n",
    "    active_sum = f + g\n",
    "    if active_sum == 0: return 50.0, 0.0\n",
    "    \n",
    "    sentiment_direction = (g - f) / active_sum\n",
    "    emotion_density = np.sqrt(active_sum / tw) \n",
    "    \n",
    "    # [조정] 증폭 계수를 2.5로 하향 (3.5는 너무 들쭉날쭉함)\n",
    "    scaled_score = np.tanh(sentiment_direction * emotion_density * 2.5)\n",
    "    fng_index = (scaled_score + 1.0) * 50.0\n",
    "    \n",
    "    return fng_index, emotion_density\n",
    "\n",
    "# 일일 원본 데이터 생성\n",
    "daily_rows = []\n",
    "for d in pd.date_range(start_d, end_d, freq=\"D\").date:\n",
    "    if d not in daily_stats: continue\n",
    "    idx, dens = calculate_raw_index(daily_stats[d])\n",
    "    daily_rows.append({\n",
    "        \"date\": d,\n",
    "        \"fng_raw\": idx,\n",
    "        \"emotion_density\": dens\n",
    "    })\n",
    "\n",
    "df_daily = pd.DataFrame(daily_rows)\n",
    "\n",
    "# [핵심] 3일 이동 평균 적용: 일일 노이즈를 제거하여 흐름을 부드럽게 만듦\n",
    "df_daily['fng_index'] = df_daily['fng_raw'].rolling(window=3, min_periods=1, center=True).mean().round(2)\n",
    "\n",
    "# 저장\n",
    "df_daily[['date', 'fng_index', 'emotion_density']].to_csv(OUT_DAILY_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# --- 주간 데이터 처리 ---\n",
    "df_daily['date'] = pd.to_datetime(df_daily['date'])\n",
    "df_weekly = df_daily.resample('W-MON', on='date').mean().reset_index()\n",
    "df_weekly['fng_index'] = df_weekly['fng_index'].round(2)\n",
    "df_weekly.to_csv(OUT_WEEKLY_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n[완료] 지수 산출 결과가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a98a0",
   "metadata": {},
   "source": [
    "# 모델 추가학습 및 디시인사이드 최종 지수 산출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfd75657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 사용 중인 장치: cuda\n",
      "데이터셋 준비 중...\n",
      "CSV 데이터 라벨링 중...\n",
      "토크나이징 중...\n",
      "모델 로드 중...\n",
      "학습 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1020' max='1020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1020/1020 00:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.458737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.347841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.244700</td>\n",
       "      <td>0.408167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료: ./finetuned_stock_bert\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# 1. 설정 및 장치 확인\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"현재 사용 중인 장치: {device}\")\n",
    "\n",
    "# 2. 감성 단어 리스트 (최신 업데이트 버전)\n",
    "fear_words = [\n",
    "    '공포', '하락', '인버스', '패닉', '셀링', '손절', '풀숏', '숏', '불안', '위험', '회피', \n",
    "    '리스크', '침체', '변동성', '마이너스', '물타기', '패닉셀', '매도', '존버', '익절', \n",
    "    '조정', '도망', '포기', '이탈', '환장', '한강', '자살', '수온', '음전', '설거지', \n",
    "    '개미털이', '공매도', '떡락', '탈주', '금리인상', '추락', '피바다', '반등없음', '쫄림', \n",
    "    '절망', '멘붕', '현금확보', '지옥', '외인매도', '기관매도', '양아치', '사기', '물림', \n",
    "    '손실확정', '악재', '청산', '잡주', '빠지', '망하', '좆', '새끼', '죽', '반대', \n",
    "    '힘들', '조심', '급락', '나쁘', '당하', '병신', '버블', '탈출', '끝물', '구조대', \n",
    "    '물리', '관세', '밀리', '내리', '떨어지', '음봉', '하향', '부진', '부담', '우려', \n",
    "    '실망', '비싸', '관망', '박스권', '고평가', '거품', '팔', '지랄', '좃', '던지', '버스', \n",
    "    '뒤지', '처박', '실패', '시발', '무섭', '공포장',\n",
    "    '폭락', '박살', '꼬라지', '무너짐', '손실중'\n",
    "]\n",
    "greed_words = [\n",
    "    '탐욕', '레버리지', '풀롱', '롱', '수출호재', '호재', '기대', '떡상', '드가자', '가즈아', \n",
    "    '매수', '신고가', '고점', '몰빵', '상승', '폭등', '불장', '랠리', 'FOMO', '포모', \n",
    "    '효자', '불타기', '과열', '쭉쭉', '급등주', '급등', '추매', '최고', '대장', '사랑', \n",
    "    '감사', '갓전자', '갓하이닉스', '갓현대', '갓차', '개미승리', '상방확정', '호재반영', \n",
    "    '롱진입', '외인매수', '기관매수', '마진확대', '우상향', '폭주기관차', '상승장', '행복', \n",
    "    '안착', '성공', '부럽', '커피값', '저녁값', '소고기', '광기', '회복', '영끌', '빚투', \n",
    "    '축하', '와우', '대박', '추격매수', '전고점', '매집중', '돌파', '수익', '반등', '양봉', \n",
    "    '실현', '베팅', '홀딩', '기대감', '차익', '부자', '강하', '수혜', '강세', '성장', \n",
    "    '사이클', '수급', '오르', '이익', '기회', '모으', '수주', '상방', '가치', '수익중',\n",
    "    '갓하이닉스', '불기둥', '추가상승'\n",
    "]\n",
    "\n",
    "# 3. 데이터 로드 및 라벨링 함수 (CSV 버전)\n",
    "def prepare_dataset_from_csv(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"파일을 찾을 수 없습니다: {csv_path}\")\n",
    "    \n",
    "    # CSV 읽기 (인코딩 주의)\n",
    "    df_raw = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "    \n",
    "    data = []\n",
    "    print(\"CSV 데이터 라벨링 중...\")\n",
    "    for _, row in df_raw.iterrows():\n",
    "        # 제목과 본문 추출 및 결합\n",
    "        title = str(row.get('title', '')).strip()\n",
    "        content = str(row.get('content', '')).strip()\n",
    "        \n",
    "        texts = [title, content]\n",
    "        \n",
    "        for text in texts:\n",
    "            if len(text) < 5: continue # 너무 짧은 텍스트 제외\n",
    "            \n",
    "            f_score = sum(1 for w in fear_words if w in text)\n",
    "            g_score = sum(1 for w in greed_words if w in text)\n",
    "            \n",
    "            # 라벨링: 0(공포), 1(중립), 2(탐욕)\n",
    "            label = 0 if f_score > g_score else (2 if g_score > f_score else 1)\n",
    "            data.append({'text': text, 'label': label})\n",
    "            \n",
    "    return pd.DataFrame(data).drop_duplicates()\n",
    "\n",
    "# 4. 데이터셋 클래스 정의\n",
    "class StockDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 5. 실행 로직\n",
    "INPUT_CSV = \"../data/posts_hynix_with_comment.csv\" # 입력 파일 경로\n",
    "\n",
    "print(\"데이터셋 준비 중...\")\n",
    "df = prepare_dataset_from_csv(INPUT_CSV)\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, stratify=df['label'], random_state=42)\n",
    "\n",
    "MODEL_NAME = \"snunlp/KR-FinBert-SC\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"토크나이징 중...\")\n",
    "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_dataset = StockDataset(train_encodings, train_df['label'].tolist())\n",
    "val_dataset = StockDataset(val_encodings, val_df['label'].tolist())\n",
    "\n",
    "# 6. 모델 학습 설정\n",
    "print(\"모델 로드 중...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./stock_model_checkpoints',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"학습 시작...\")\n",
    "trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "save_path = \"./finetuned_stock_bert\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"모델 저장 완료: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f94493f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로딩 중... (장치: cuda)\n",
      "데이터 분석 및 지수 산출 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3367/3367 [00:27<00:00, 120.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[완료] 일별 지수 생성 성공!\n",
      "- 저장 경로: ..\\output\\hynix_fng_balanced.csv\n",
      "- 총 분석 일수: 320일\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# =========================\n",
    "# 1. 설정 및 경로\n",
    "# =========================\n",
    "# 분석할 파일 경로 (삼성전자의 경우 posts_samsung_with_comment.csv 등으로 변경 가능)\n",
    "CSV_PATH = r\"..\\data\\posts_hynix_with_comment.csv\" \n",
    "OUT_DIR = r\"..\\output\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 저장될 파일명 (종목에 따라 이름을 바꿔서 저장하세요)\n",
    "OUT_DAILY_CSV = os.path.join(OUT_DIR, \"hynix_fng_balanced.csv\")\n",
    "\n",
    "# 학습시킨 모델 경로\n",
    "MODEL_NAME = \"./finetuned_stock_bert\" \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# =========================\n",
    "# 2. 날짜 통일 함수 (모든 형식 대응)\n",
    "# =========================\n",
    "def parse_ultimate_date(date_str):\n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    # 1. 시간 형식 (18:31 등)은 분석 제외\n",
    "    if re.match(r'^\\d{1,2}:\\d{2}$', date_str):\n",
    "        return None\n",
    "\n",
    "    # 2. '작년' 키워드 처리\n",
    "    if '작년' in date_str:\n",
    "        return datetime(2025, 12, 31).date()\n",
    "\n",
    "    # 3. YY.MM.DD 형식 (예: 25.12.31) 처리\n",
    "    long_date_match = re.search(r'(\\d{2,4})\\.(\\d{1,2})\\.(\\d{1,2})', date_str)\n",
    "    if long_date_match:\n",
    "        year = int(long_date_match.group(1))\n",
    "        month = int(long_date_match.group(2))\n",
    "        day = int(long_date_match.group(3))\n",
    "        \n",
    "        if year < 100: year += 2000 # 25 -> 2025 변환\n",
    "        try:\n",
    "            return datetime(year, month, day).date()\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    # 4. MM.DD 형식 (예: 01.12) 처리 -> 2026년으로 고정\n",
    "    short_date_match = re.search(r'^(\\d{1,2})\\.(\\d{1,2})$', date_str)\n",
    "    if short_date_match:\n",
    "        month = int(short_date_match.group(1))\n",
    "        day = int(short_date_match.group(2))\n",
    "        try:\n",
    "            return datetime(2026, month, day).date()\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    # 5. 기타 표준 날짜 형식 시도\n",
    "    try:\n",
    "        dt = pd.to_datetime(date_str, errors='coerce')\n",
    "        if pd.notna(dt):\n",
    "            return dt.date()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return None\n",
    "\n",
    "# =========================\n",
    "# 3. 모델 로드 및 추론 로직\n",
    "# =========================\n",
    "print(f\"모델 로딩 중... (장치: {DEVICE})\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "def infer_one(text: str):\n",
    "    if not text or len(str(text)) < 2: return None\n",
    "    \n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, max_length=128).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    probs = torch.softmax(logits, dim=-1).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # 레이블 정의: 0(Fear), 1(Neutral), 2(Greed)\n",
    "    # 중립(Neutral) 확률이 60%를 넘으면 중립으로 처리하여 노이즈 제거\n",
    "    if probs[1] > 0.60: \n",
    "        return \"neutral\"\n",
    "    return \"fear\" if probs[0] > probs[2] else \"greed\"\n",
    "\n",
    "# =========================\n",
    "# 4. 데이터 로드 및 일별 집계\n",
    "# =========================\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    print(f\"파일을 찾을 수 없습니다: {CSV_PATH}\")\n",
    "else:\n",
    "    df_raw = pd.read_csv(CSV_PATH, encoding='utf-8-sig')\n",
    "    daily_stats = defaultdict(lambda: {\"fear\": 0, \"neutral\": 0, \"greed\": 0, \"total_w\": 0.0})\n",
    "\n",
    "    print(\"데이터 분석 및 지수 산출 시작...\")\n",
    "    for _, row in tqdm(df_raw.iterrows(), total=len(df_raw)):\n",
    "        # 날짜 정제\n",
    "        d_date = parse_ultimate_date(row.get('date', ''))\n",
    "        if d_date is None: continue # 시간 형식 등은 건너뜀\n",
    "        \n",
    "        # 제목과 본문 텍스트 추출\n",
    "        texts = [('제목', row.get(\"title\", \"\")), ('본문', row.get(\"content\", \"\"))]\n",
    "        \n",
    "        for t_type, t in texts:\n",
    "            label = infer_one(t)\n",
    "            if label:\n",
    "                # 제목은 주가 영향력이 크므로 1.5배 가중치 부여\n",
    "                weight = 1.5 if t_type == '제목' else 1.0\n",
    "                daily_stats[d_date][label] += weight\n",
    "                daily_stats[d_date][\"total_w\"] += weight\n",
    "\n",
    "    # =========================\n",
    "    # 5. 결과 계산 및 저장\n",
    "    # =========================\n",
    "    daily_rows = []\n",
    "    # 날짜 순으로 정렬하여 계산\n",
    "    for d in sorted(daily_stats.keys()):\n",
    "        stats = daily_stats[d]\n",
    "        f, g, n, tw = stats[\"fear\"], stats[\"greed\"], stats[\"neutral\"], stats[\"total_w\"]\n",
    "        \n",
    "        # 감정 데이터가 없는 날은 중립(50점) 처리\n",
    "        if tw == 0 or (f + g) == 0:\n",
    "            idx, dens = 50.0, 0.0\n",
    "        else:\n",
    "            # 심리 방향성 (-1 ~ 1)\n",
    "            sentiment_direction = (g - f) / (f + g)\n",
    "            # 감정 밀도: 전체 글 중 감정 섞인 글의 비중\n",
    "            emotion_density = np.sqrt((f + g) / tw)\n",
    "            \n",
    "            # 최종 지수 (0 ~ 100)\n",
    "            scaled_score = np.tanh(sentiment_direction * emotion_density * 2.5)\n",
    "            idx = (scaled_score + 1.0) * 50.0\n",
    "            dens = emotion_density\n",
    "\n",
    "        daily_rows.append({\n",
    "            \"date\": d,\n",
    "            \"fng_raw\": round(idx, 2),\n",
    "            \"emotion_density\": round(dens, 4)\n",
    "        })\n",
    "\n",
    "    if daily_rows:\n",
    "        df_daily = pd.DataFrame(daily_rows)\n",
    "        \n",
    "        # [추가] 3일 이동 평균(Rolling)을 적용하여 차트를 부드럽게 만듦\n",
    "        df_daily['fng_index'] = df_daily['fng_raw'].rolling(window=3, min_periods=1, center=True).mean().round(2)\n",
    "\n",
    "        # 필요한 컬럼만 저장\n",
    "        df_daily[['date', 'fng_index', 'emotion_density']].to_csv(OUT_DAILY_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(f\"\\n[완료] 일별 지수 생성 성공!\")\n",
    "        print(f\"- 저장 경로: {OUT_DAILY_CSV}\")\n",
    "        print(f\"- 총 분석 일수: {len(df_daily)}일\")\n",
    "    else:\n",
    "        print(\"\\n[알림] 조건에 맞는 데이터가 없어 결과가 저장되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f0edb",
   "metadata": {},
   "source": [
    "# FMkorea 제목, 본문, 댓글 리스트 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1ff943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeon\\AppData\\Local\\Temp\\ipykernel_26068\\3261695241.py:9: DeprecationWarning: behavior of `num_workers=0` is changed since v0.21.0. If you want to keep the previous behavior, please set `num_workers=-1`.\n",
      "  kiwi = Kiwi(num_workers=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 로드 중...\n",
      "의미 단어 추출 중 (Kiwi)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [00:12<00:00, 89.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[완료]\n",
      "- 추출 결과 저장됨: fmkorea_meaningful_results.csv\n",
      "- 단어 빈도(중복) 리스트 저장됨: fmkorea_word_counts.csv\n",
      "- 고유 의미 단어 수: 15440개\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Kiwi 초기화\n",
    "kiwi = Kiwi(num_workers=0)\n",
    "\n",
    "# 2. 추출할 의미 있는 품사 정의 (실질 형태소)\n",
    "# NNG(일반 명사), NNP(고유 명사), VV(동사), VA(형용사), XR(어근), SL(외국어)\n",
    "MEANINGFUL_TAGS = {'NNG', 'NNP', 'VV', 'VA', 'XR', 'SL'}\n",
    "\n",
    "def tokenize_meaningful_only(input_path, output_csv, vocab_csv):\n",
    "    all_processed_data = []\n",
    "    word_counter = Counter() # 단어 중복 빈도 체크용\n",
    "    \n",
    "    print(\"파일 로드 중...\")\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    print(\"의미 단어 추출 중 (Kiwi)...\")\n",
    "    for line in tqdm(lines):\n",
    "        try:\n",
    "            post = json.loads(line)\n",
    "            texts = [('제목', post.get('title', '')), ('본문', post.get('content', ''))]\n",
    "            for c in post.get('comments', []) or []:\n",
    "                if isinstance(c, dict): texts.append(('댓글', c.get('comment', '')))\n",
    "\n",
    "            for content_type, text in texts:\n",
    "                if not text or len(str(text).strip()) < 2: continue\n",
    "                \n",
    "                # Kiwi 토큰화\n",
    "                tokens = kiwi.tokenize(text)\n",
    "                \n",
    "                # 의미 있는 품사만 필터링하여 단어 리스트 생성\n",
    "                meaningful_words = [t.form for t in tokens if t.tag in MEANINGFUL_TAGS]\n",
    "                \n",
    "                # 전체 단어 빈도수 업데이트 (얼마나 겹치는지 확인용)\n",
    "                word_counter.update(meaningful_words)\n",
    "                \n",
    "                # 결과 정리\n",
    "                meaningful_str = \" \".join(meaningful_words)\n",
    "                all_processed_data.append({\n",
    "                    \"date\": post.get(\"date\", \"\"),\n",
    "                    \"type\": content_type,\n",
    "                    \"original_text\": text.replace(\"\\n\", \" \"),\n",
    "                    \"meaningful_words\": meaningful_str\n",
    "                })\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 3-1. 추출 데이터 저장 (원문 + 의미 단어)\n",
    "    df = pd.DataFrame(all_processed_data)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # 3-2. 단어 빈도 데이터 저장 (얼마나 겹치는지 보여주는 리스트)\n",
    "    vocab_df = pd.DataFrame(word_counter.most_common(), columns=['word', 'count'])\n",
    "    vocab_df.to_csv(vocab_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(f\"\\n[완료]\")\n",
    "    print(f\"- 추출 결과 저장됨: {output_csv}\")\n",
    "    print(f\"- 단어 빈도(중복) 리스트 저장됨: {vocab_csv}\")\n",
    "    print(f\"- 고유 의미 단어 수: {len(vocab_df)}개\")\n",
    "\n",
    "# 실행\n",
    "JSONL_PATH = \"../data/fmkorea_hynix_hot_posts.jsonl\"\n",
    "RESULT_CSV = \"fmkorea_meaningful_results.csv\"\n",
    "VOCAB_CSV = \"fmkorea_word_counts.csv\"\n",
    "\n",
    "tokenize_meaningful_only(JSONL_PATH, RESULT_CSV, VOCAB_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274eac6b",
   "metadata": {},
   "source": [
    "# 디시인사이드 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f40ad8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeon\\AppData\\Local\\Temp\\ipykernel_26068\\2390522692.py:8: DeprecationWarning: behavior of `num_workers=0` is changed since v0.21.0. If you want to keep the previous behavior, please set `num_workers=-1`.\n",
      "  kiwi = Kiwi(num_workers=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일 로드 중...\n",
      "제목 및 본문 의미 단어 추출 중 (Kiwi)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3367/3367 [00:02<00:00, 1318.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[완료]\n",
      "- 추출 결과 저장됨: fmkorea_meaningful_results_csv.csv\n",
      "- 단어 빈도 리스트 저장됨: fmkorea_word_counts_csv.csv\n",
      "- 고유 의미 단어 수: 6547개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Kiwi 초기화\n",
    "kiwi = Kiwi(num_workers=0)\n",
    "\n",
    "# 2. 추출할 의미 있는 품사 정의\n",
    "MEANINGFUL_TAGS = {'NNG', 'NNP', 'VV', 'VA', 'XR', 'SL'}\n",
    "\n",
    "def tokenize_csv_title_content(input_path, output_csv, vocab_csv):\n",
    "    word_counter = Counter()\n",
    "    processed_results = []\n",
    "\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"오류: 파일을 찾을 수 없습니다: {input_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"CSV 파일 로드 중...\")\n",
    "    # CSV 파일을 읽어옵니다. (인코딩 에러 방지를 위해 utf-8-sig 사용)\n",
    "    df_raw = pd.read_csv(input_path, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"제목 및 본문 의미 단어 추출 중 (Kiwi)...\")\n",
    "    # DataFrame의 각 행을 순회합니다.\n",
    "    for _, row in tqdm(df_raw.iterrows(), total=len(df_raw)):\n",
    "        try:\n",
    "            # 제목과 본문 컬럼 데이터 가져오기 (컬럼명이 다를 경우 여기서 수정)\n",
    "            title = str(row.get('title', ''))\n",
    "            content = str(row.get('content', ''))\n",
    "            date = str(row.get('date', ''))\n",
    "            \n",
    "            texts = [('제목', title), ('본문', content)]\n",
    "\n",
    "            for content_type, text in texts:\n",
    "                if not text or len(text.strip()) < 2:\n",
    "                    continue\n",
    "                \n",
    "                # Kiwi 토큰화\n",
    "                tokens = kiwi.tokenize(text)\n",
    "                \n",
    "                # 의미 있는 품사만 필터링\n",
    "                meaningful_words = [t.form for t in tokens if t.tag in MEANINGFUL_TAGS]\n",
    "                \n",
    "                # 단어 빈도수 업데이트\n",
    "                word_counter.update(meaningful_words)\n",
    "                \n",
    "                # 결과 정리\n",
    "                meaningful_str = \" \".join(meaningful_words)\n",
    "                processed_results.append({\n",
    "                    \"date\": date,\n",
    "                    \"type\": content_type,\n",
    "                    \"original_text\": text.replace(\"\\n\", \" \"),\n",
    "                    \"meaningful_words\": meaningful_str\n",
    "                })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # 3. 결과 저장\n",
    "    if processed_results:\n",
    "        # 추출 결과 저장\n",
    "        res_df = pd.DataFrame(processed_results)\n",
    "        res_df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        # 단어 빈도 저장\n",
    "        vocab_df = pd.DataFrame(word_counter.most_common(), columns=['word', 'count'])\n",
    "        vocab_df.to_csv(vocab_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        \n",
    "        print(f\"\\n[완료]\")\n",
    "        print(f\"- 추출 결과 저장됨: {output_csv}\")\n",
    "        print(f\"- 단어 빈도 리스트 저장됨: {vocab_csv}\")\n",
    "        print(f\"- 고유 의미 단어 수: {len(vocab_df)}개\")\n",
    "    else:\n",
    "        print(\"\\n[알림] 추출된 데이터가 없습니다.\")\n",
    "\n",
    "# 실행 설정\n",
    "# 업로드하신 posts_samsung_with_comment.csv 파일 경로로 설정하세요.\n",
    "INPUT_CSV = \"../data/posts_hynix_with_comment.csv\" \n",
    "RESULT_CSV = \"fmkorea_meaningful_results_csv.csv\"\n",
    "VOCAB_CSV = \"fmkorea_word_counts_csv.csv\"\n",
    "\n",
    "tokenize_csv_title_content(INPUT_CSV, RESULT_CSV, VOCAB_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
